"""
GitHub Actions Simulator - æ”¹è‰¯ã•ã‚ŒãŸActWrapper
è¨ºæ–­æ©Ÿèƒ½ã€æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ã€ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºæ©Ÿèƒ½ã‚’æŒã¤
EnhancedActWrapperã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
"""

from __future__ import annotations

import json
import os
import subprocess
import threading
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .act_wrapper import ActWrapper
from .auto_recovery import AutoRecovery, RecoverySession, FallbackExecutionResult
from .diagnostic import DiagnosticService, DiagnosticResult, DiagnosticStatus
from .docker_integration_checker import DockerIntegrationChecker, DockerConnectionStatus
from .execution_tracer import ExecutionTracer, ExecutionStage
from .hangup_detector import HangupDetector, HangupAnalysis, ErrorReport, DebugBundle
from .logger import ActionsLogger

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys

sys.path.append(str(Path(__file__).parent.parent.parent / "src"))
from performance_monitor import PerformanceMonitor


class DeadlockType(Enum):
    """ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯ã®ç¨®é¡"""

    STDOUT_THREAD = "stdout_thread"
    STDERR_THREAD = "stderr_thread"
    PROCESS_WAIT = "process_wait"
    DOCKER_COMMUNICATION = "docker_communication"
    RESOURCE_EXHAUSTION = "resource_exhaustion"


@dataclass
class DeadlockIndicator:
    """ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã®æŒ‡æ¨™"""

    deadlock_type: DeadlockType
    detected_at: str = field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )
    thread_name: Optional[str] = None
    process_pid: Optional[int] = None
    details: Dict[str, Any] = field(default_factory=dict)
    severity: str = "HIGH"  # HIGH, MEDIUM, LOW
    recommendations: List[str] = field(default_factory=list)


@dataclass
class MonitoredProcess:
    """ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹ã®æƒ…å ±"""

    process: subprocess.Popen
    command: List[str]
    start_time: float
    stdout_thread: Optional[threading.Thread] = None
    stderr_thread: Optional[threading.Thread] = None
    stdout_lines: List[str] = field(default_factory=list)
    stderr_lines: List[str] = field(default_factory=list)
    last_activity: float = field(default_factory=time.time)
    deadlock_indicators: List[DeadlockIndicator] = field(default_factory=list)
    force_killed: bool = False


@dataclass
class DetailedResult:
    """è©³ç´°ãªå®Ÿè¡Œçµæœ"""

    success: bool
    returncode: int
    stdout: str
    stderr: str
    command: str
    execution_time_ms: float
    diagnostic_results: List[DiagnosticResult] = field(default_factory=list)
    deadlock_indicators: List[DeadlockIndicator] = field(default_factory=list)
    process_monitoring_data: Dict[str, Any] = field(default_factory=dict)
    hang_analysis: Optional[HangupAnalysis] = None
    error_report: Optional[ErrorReport] = None
    performance_metrics: Optional[Dict[str, Any]] = None
    bottlenecks_detected: List[Dict[str, Any]] = field(default_factory=list)
    optimization_opportunities: List[Dict[str, Any]] = field(default_factory=list)


class ProcessMonitor:
    """
    æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
    ç´°ã‹ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶å¾¡ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æä¾›
    """

    def __init__(
        self,
        logger: Optional[ActionsLogger] = None,
        deadlock_detection_interval: float = 10.0,
        activity_timeout: float = 60.0,
        warning_timeout: float = 480.0,  # 8åˆ†ã§è­¦å‘Š
        escalation_timeout: float = 540.0,  # 9åˆ†ã§ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        heartbeat_interval: float = 30.0,
        detailed_logging: bool = True,
    ):
        """
        ProcessMonitorã‚’åˆæœŸåŒ–

        Args:
            logger: ãƒ­ã‚°å‡ºåŠ›ç”¨ã®ãƒ­ã‚¬ãƒ¼
            deadlock_detection_interval: ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã®é–“éš”ï¼ˆç§’ï¼‰
            activity_timeout: ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            warning_timeout: è­¦å‘Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            escalation_timeout: ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            heartbeat_interval: ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé–“éš”ï¼ˆç§’ï¼‰
            detailed_logging: è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.logger = logger or ActionsLogger(verbose=True)
        self.deadlock_detection_interval = deadlock_detection_interval
        self.activity_timeout = activity_timeout
        self.warning_timeout = warning_timeout
        self.escalation_timeout = escalation_timeout
        self.heartbeat_interval = heartbeat_interval
        self.detailed_logging = detailed_logging

        # ç›£è¦–çŠ¶æ…‹
        self._monitoring_active = False
        self._monitor_thread: Optional[threading.Thread] = None
        self._stop_event = threading.Event()

        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹
        self._warning_sent = False
        self._escalation_started = False
        self._last_heartbeat = time.time()

        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
        self._resource_snapshots: List[Dict[str, Any]] = []
        self._performance_metrics: Dict[str, float] = {}

    def monitor_with_heartbeat(
        self, monitored_process: MonitoredProcess, timeout: int
    ) -> Tuple[bool, List[DeadlockIndicator]]:
        """
        æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç›£è¦–
        ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè­¦å‘Š -> å¼·åˆ¶çµ‚äº†ï¼‰ã‚’å®Ÿè£…

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            timeout: æœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            Tuple[bool, List[DeadlockIndicator]]: (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ•ãƒ©ã‚°, ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™ãƒªã‚¹ãƒˆ)
        """
        self.logger.info(
            f"æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚’é–‹å§‹: PID {monitored_process.process.pid}, ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’"
        )

        start_time = time.time()
        self._last_heartbeat = start_time

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ®µéšã®è¨­å®š
        warning_deadline = (
            start_time + self.warning_timeout if self.warning_timeout > 0 else None
        )
        escalation_deadline = (
            start_time + self.escalation_timeout
            if self.escalation_timeout > 0
            else None
        )
        final_deadline = start_time + timeout if timeout > 0 else None

        next_heartbeat = start_time + self.heartbeat_interval
        next_resource_check = start_time + 10.0  # 10ç§’ã”ã¨ã«ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯

        # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        self._warning_sent = False
        self._escalation_started = False

        # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        self._start_deadlock_detection(monitored_process)

        try:
            while True:
                return_code = monitored_process.process.poll()
                if return_code is not None:
                    elapsed = time.time() - start_time
                    self.logger.info(
                        f"ãƒ—ãƒ­ã‚»ã‚¹ãŒæ­£å¸¸çµ‚äº†: PID {monitored_process.process.pid}, çµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code}, å®Ÿè¡Œæ™‚é–“: {elapsed:.2f}ç§’"
                    )
                    break

                now = time.time()
                elapsed = now - start_time

                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†
                timeout_result = self._handle_timeout_escalation(
                    monitored_process,
                    now,
                    elapsed,
                    warning_deadline,
                    escalation_deadline,
                    final_deadline,
                )

                if timeout_result:
                    return timeout_result

                # æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒ­ã‚°
                if now >= next_heartbeat:
                    self._log_enhanced_heartbeat(monitored_process, elapsed)
                    next_heartbeat = now + self.heartbeat_interval
                    self._last_heartbeat = now

                # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                if now >= next_resource_check:
                    self._check_resource_usage(monitored_process)
                    next_resource_check = now + 10.0

                # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºçµæœã‚’ãƒã‚§ãƒƒã‚¯
                if monitored_process.deadlock_indicators:
                    self.logger.warning(
                        f"ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: PID {monitored_process.process.pid}"
                    )
                    return True, monitored_process.deadlock_indicators

                time.sleep(1)

            return False, monitored_process.deadlock_indicators

        finally:
            # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã‚’åœæ­¢
            self._stop_deadlock_detection()
            # æœ€çµ‚ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’è¨˜éŒ²
            self._record_final_metrics(monitored_process, time.time() - start_time)

    def detect_deadlock_conditions(
        self, monitored_process: MonitoredProcess
    ) -> List[DeadlockIndicator]:
        """
        ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¡ä»¶ã‚’æ¤œå‡º

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹

        Returns:
            List[DeadlockIndicator]: æ¤œå‡ºã•ã‚ŒãŸãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™ã®ãƒªã‚¹ãƒˆ
        """
        indicators = []
        now = time.time()

        # ã‚¹ãƒ¬ãƒƒãƒ‰ã®å¿œç­”æ€§ãƒã‚§ãƒƒã‚¯
        if monitored_process.stdout_thread:
            if not self._is_thread_responsive(monitored_process.stdout_thread):
                indicators.append(
                    DeadlockIndicator(
                        deadlock_type=DeadlockType.STDOUT_THREAD,
                        thread_name=monitored_process.stdout_thread.name,
                        process_pid=monitored_process.process.pid,
                        details={
                            "thread_alive": monitored_process.stdout_thread.is_alive(),
                            "last_activity": monitored_process.last_activity,
                        },
                        recommendations=[
                            "æ¨™æº–å‡ºåŠ›ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå¿œç­”ã—ã¾ã›ã‚“",
                            "ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                        ],
                    )
                )

        if monitored_process.stderr_thread:
            if not self._is_thread_responsive(monitored_process.stderr_thread):
                indicators.append(
                    DeadlockIndicator(
                        deadlock_type=DeadlockType.STDERR_THREAD,
                        thread_name=monitored_process.stderr_thread.name,
                        process_pid=monitored_process.process.pid,
                        details={
                            "thread_alive": monitored_process.stderr_thread.is_alive(),
                            "last_activity": monitored_process.last_activity,
                        },
                        recommendations=[
                            "æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå¿œç­”ã—ã¾ã›ã‚“",
                            "ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                        ],
                    )
                )

        # ãƒ—ãƒ­ã‚»ã‚¹ã®å¿œç­”æ€§ãƒã‚§ãƒƒã‚¯
        if now - monitored_process.last_activity > self.activity_timeout:
            indicators.append(
                DeadlockIndicator(
                    deadlock_type=DeadlockType.PROCESS_WAIT,
                    process_pid=monitored_process.process.pid,
                    details={
                        "inactive_duration": now - monitored_process.last_activity,
                        "timeout_threshold": self.activity_timeout,
                    },
                    recommendations=[
                        f"ãƒ—ãƒ­ã‚»ã‚¹ãŒ{self.activity_timeout}ç§’é–“éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã§ã™",
                        "Dockeré€šä¿¡ã®å•é¡Œã¾ãŸã¯ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                    ],
                )
            )

        # ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡ã®æ¤œå‡º
        try:
            import psutil

            process = psutil.Process(monitored_process.process.pid)
            memory_percent = process.memory_percent()
            cpu_percent = process.cpu_percent()

            if memory_percent > 90.0:
                indicators.append(
                    DeadlockIndicator(
                        deadlock_type=DeadlockType.RESOURCE_EXHAUSTION,
                        process_pid=monitored_process.process.pid,
                        details={
                            "memory_percent": memory_percent,
                            "cpu_percent": cpu_percent,
                        },
                        severity="HIGH",
                        recommendations=[
                            f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒç•°å¸¸ã«é«˜ã„ã§ã™: {memory_percent:.1f}%",
                            "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã¾ãŸã¯ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                        ],
                    )
                )

        except (ImportError, Exception):
            # psutilãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            pass

        return indicators

    def force_cleanup_on_timeout(self, monitored_process: MonitoredProcess) -> None:
        """
        æ”¹è‰¯ã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        æ®µéšçš„ãªãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã¨ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ã‚’ä¿è¨¼

        Args:
            monitored_process: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
        """
        self.logger.warning(
            f"æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹: PID {monitored_process.process.pid}"
        )

        cleanup_start = time.time()

        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ç©ã‚„ã‹ãªçµ‚äº†ã‚’è©¦è¡Œ (SIGTERM)
            if monitored_process.process.poll() is None:
                self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: SIGTERMã§ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚’è©¦è¡Œ")
                monitored_process.process.terminate()

                # 5ç§’å¾…æ©Ÿ
                try:
                    monitored_process.process.wait(timeout=5)
                    self.logger.info("ãƒ—ãƒ­ã‚»ã‚¹ãŒSIGTERMã§æ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸ")
                    return
                except subprocess.TimeoutExpired:
                    self.logger.warning(
                        "SIGTERMå¾Œã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã¿ã¾ã™"
                    )

            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’çµ‚äº†
            if monitored_process.process.poll() is None:
                self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®çµ‚äº†ã‚’è©¦è¡Œ")
                try:
                    import os
                    import signal

                    if hasattr(os, "killpg"):
                        os.killpg(
                            os.getpgid(monitored_process.process.pid), signal.SIGTERM
                        )
                        time.sleep(2)

                        if monitored_process.process.poll() is None:
                            os.killpg(
                                os.getpgid(monitored_process.process.pid),
                                signal.SIGKILL,
                            )
                except (OSError, ProcessLookupError):
                    self.logger.debug(
                        "ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—çµ‚äº†ã«å¤±æ•—ã€å€‹åˆ¥ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã«é€²ã¿ã¾ã™"
                    )

            # ã‚¹ãƒ†ãƒƒãƒ—3: å¼·åˆ¶çµ‚äº† (SIGKILL)
            if monitored_process.process.poll() is None:
                self.logger.warning("ã‚¹ãƒ†ãƒƒãƒ—3: SIGKILLã§ãƒ—ãƒ­ã‚»ã‚¹å¼·åˆ¶çµ‚äº†")
                monitored_process.process.kill()
                monitored_process.force_killed = True

                # æœ€çµ‚ç¢ºèª
                try:
                    monitored_process.process.wait(timeout=3)
                    self.logger.info("ãƒ—ãƒ­ã‚»ã‚¹ãŒSIGKILLã§å¼·åˆ¶çµ‚äº†ã•ã‚Œã¾ã—ãŸ")
                except subprocess.TimeoutExpired:
                    self.logger.error(
                        "ãƒ—ãƒ­ã‚»ã‚¹ã®å¼·åˆ¶çµ‚äº†ã«å¤±æ•—ã—ã¾ã—ãŸ - ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
                    )

        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¹ãƒ¬ãƒƒãƒ‰ã¨ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.logger.info("ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¹ãƒ¬ãƒƒãƒ‰ã¨ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
        self._cleanup_threads(monitored_process)

        # ã‚¹ãƒ†ãƒƒãƒ—5: å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ã®ã‚¯ãƒªã‚¢
        self._clear_output_buffers(monitored_process)

        cleanup_duration = time.time() - cleanup_start
        self.logger.info(
            f"ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: å®Ÿè¡Œæ™‚é–“ {cleanup_duration:.2f}ç§’"
        )

    def _handle_timeout_escalation(
        self,
        monitored_process: MonitoredProcess,
        current_time: float,
        elapsed: float,
        warning_deadline: Optional[float],
        escalation_deadline: Optional[float],
        final_deadline: Optional[float],
    ) -> Optional[Tuple[bool, List[DeadlockIndicator]]]:
        """
        ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            current_time: ç¾åœ¨æ™‚åˆ»
            elapsed: çµŒéæ™‚é–“
            warning_deadline: è­¦å‘Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            escalation_deadline: ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            final_deadline: æœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

        Returns:
            Optional[Tuple[bool, List[DeadlockIndicator]]]: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®çµæœ
        """
        # è­¦å‘Šæ®µéš
        if (
            warning_deadline
            and current_time >= warning_deadline
            and not self._warning_sent
        ):
            self._warning_sent = True
            self.logger.warning(
                f"âš ï¸  ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œè­¦å‘Š: {elapsed:.1f}ç§’çµŒé (PID: {monitored_process.process.pid})\n"
                f"   - è­¦å‘Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {self.warning_timeout}ç§’\n"
                f"   - æœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§: {final_deadline - current_time:.1f}ç§’\n"
                f"   - ãƒ—ãƒ­ã‚»ã‚¹ãŒé•·æ™‚é–“å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚Dockeré€šä¿¡ã‚„ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )

            # è­¦å‘Šæ™‚ã®è©³ç´°è¨ºæ–­
            self._perform_warning_diagnostics(monitored_process)

        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ®µéš
        if (
            escalation_deadline
            and current_time >= escalation_deadline
            and not self._escalation_started
        ):
            self._escalation_started = True
            self.logger.error(
                f"ğŸš¨ ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {elapsed:.1f}ç§’çµŒé (PID: {monitored_process.process.pid})\n"
                f"   - ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {self.escalation_timeout}ç§’\n"
                f"   - æœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§: {final_deadline - current_time:.1f}ç§’\n"
                f"   - å¼·åˆ¶çµ‚äº†ã®æº–å‚™ã‚’é–‹å§‹ã—ã¾ã™ã€‚"
            )

            # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã®è©³ç´°è¨ºæ–­
            self._perform_escalation_diagnostics(monitored_process)

        # æœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        if final_deadline and current_time >= final_deadline:
            self.logger.error(
                f"ğŸ’€ ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œæœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {elapsed:.1f}ç§’çµŒé (PID: {monitored_process.process.pid})\n"
                f"   - ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†ã—ã¾ã™ã€‚"
            )
            return True, monitored_process.deadlock_indicators

        return None

    def _log_enhanced_heartbeat(
        self, monitored_process: MonitoredProcess, elapsed: float
    ) -> None:
        """
        æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒ­ã‚°ã‚’å‡ºåŠ›

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            elapsed: çµŒéæ™‚é–“
        """
        # åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
        process_info = {
            "pid": monitored_process.process.pid,
            "elapsed_seconds": round(elapsed, 1),
            "return_code": monitored_process.process.poll(),
            "stdout_lines": len(monitored_process.stdout_lines),
            "stderr_lines": len(monitored_process.stderr_lines),
            "deadlock_indicators": len(monitored_process.deadlock_indicators),
            "force_killed": monitored_process.force_killed,
        }

        # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        try:
            import psutil

            process = psutil.Process(monitored_process.process.pid)
            process_info.update(
                {
                    "cpu_percent": round(process.cpu_percent(), 2),
                    "memory_mb": round(process.memory_info().rss / (1024 * 1024), 2),
                    "threads": process.num_threads(),
                    "status": process.status(),
                }
            )
        except (ImportError, psutil.NoSuchProcess, psutil.AccessDenied):
            pass

        # æ®µéšçš„ãªè©³ç´°ãƒ¬ãƒ™ãƒ«
        if elapsed < 60:
            # æœ€åˆã®1åˆ†ã¯ç°¡æ½”ã«
            self.logger.info(
                f"ğŸ’“ ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–: {elapsed:.0f}ç§’çµŒé | PID: {process_info['pid']}"
            )
        elif elapsed < 300:
            # 5åˆ†ã¾ã§ã¯ã‚„ã‚„è©³ç´°ã«
            self.logger.info(
                f"ğŸ’“ ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–: {elapsed:.0f}ç§’çµŒé | "
                f"PID: {process_info['pid']} | "
                f"å‡ºåŠ›: {process_info['stdout_lines']}è¡Œ"
            )
        else:
            # 5åˆ†ä»¥é™ã¯è©³ç´°ã«
            self.logger.info(
                f"ğŸ’“ ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ: {elapsed:.1f}ç§’çµŒé\n"
                f"   ğŸ“Š ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±: {json.dumps(process_info, ensure_ascii=False)}"
            )

        # é•·æ™‚é–“å®Ÿè¡Œã®å ´åˆã¯è¿½åŠ æƒ…å ±
        if elapsed > 300:  # 5åˆ†ä»¥ä¸Š
            self._log_long_running_analysis(monitored_process, elapsed)

    def _check_resource_usage(self, monitored_process: MonitoredProcess) -> None:
        """
        ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ç•°å¸¸ã‚’æ¤œå‡º

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
        """
        try:
            import psutil

            process = psutil.Process(monitored_process.process.pid)

            # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
            cpu_percent = process.cpu_percent()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / (1024 * 1024)
            memory_percent = process.memory_percent()

            # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜
            snapshot = {
                "timestamp": time.time(),
                "cpu_percent": cpu_percent,
                "memory_mb": memory_mb,
                "memory_percent": memory_percent,
                "threads": process.num_threads(),
            }
            self._resource_snapshots.append(snapshot)

            # å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å‰Šé™¤ï¼ˆæœ€æ–°20å€‹ã®ã¿ä¿æŒï¼‰
            if len(self._resource_snapshots) > 20:
                self._resource_snapshots = self._resource_snapshots[-20:]

            # ç•°å¸¸æ¤œå‡º
            if memory_percent > 80.0:
                self.logger.warning(
                    f"âš ï¸  é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¤œå‡º: {memory_percent:.1f}% ({memory_mb:.1f}MB)"
                )

            if cpu_percent > 90.0:
                self.logger.warning(f"âš ï¸  é«˜CPUä½¿ç”¨é‡ã‚’æ¤œå‡º: {cpu_percent:.1f}%")

            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®æ¤œå‡ºï¼ˆéå»5åˆ†é–“ã§50%ä»¥ä¸Šå¢—åŠ ï¼‰
            if len(self._resource_snapshots) >= 10:
                old_memory = self._resource_snapshots[-10]["memory_mb"]
                if memory_mb > old_memory * 1.5:
                    self.logger.warning(
                        f"âš ï¸  ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§: {old_memory:.1f}MB â†’ {memory_mb:.1f}MB"
                    )

        except (ImportError, psutil.NoSuchProcess, psutil.AccessDenied):
            pass

    def _perform_warning_diagnostics(self, monitored_process: MonitoredProcess) -> None:
        """
        è­¦å‘Šæ®µéšã§ã®è©³ç´°è¨ºæ–­

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
        """
        self.logger.info("ğŸ” è­¦å‘Šæ®µéšè¨ºæ–­ã‚’å®Ÿè¡Œä¸­...")

        # ã‚¹ãƒ¬ãƒƒãƒ‰çŠ¶æ…‹ã®ç¢ºèª
        if monitored_process.stdout_thread:
            self.logger.info(
                f"   - æ¨™æº–å‡ºåŠ›ã‚¹ãƒ¬ãƒƒãƒ‰: {'ç”Ÿå­˜' if monitored_process.stdout_thread.is_alive() else 'åœæ­¢'}"
            )
        if monitored_process.stderr_thread:
            self.logger.info(
                f"   - æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰: {'ç”Ÿå­˜' if monitored_process.stderr_thread.is_alive() else 'åœæ­¢'}"
            )

        # æœ€è¿‘ã®å‡ºåŠ›æ´»å‹•
        recent_stdout = len(monitored_process.stdout_lines)
        recent_stderr = len(monitored_process.stderr_lines)
        self.logger.info(
            f"   - å‡ºåŠ›è¡Œæ•°: stdout={recent_stdout}, stderr={recent_stderr}"
        )

        # æœ€å¾Œã®æ´»å‹•ã‹ã‚‰ã®çµŒéæ™‚é–“
        inactive_duration = time.time() - monitored_process.last_activity
        self.logger.info(f"   - æœ€å¾Œã®æ´»å‹•ã‹ã‚‰ã®çµŒéæ™‚é–“: {inactive_duration:.1f}ç§’")

    def _perform_escalation_diagnostics(
        self, monitored_process: MonitoredProcess
    ) -> None:
        """
        ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ®µéšã§ã®è©³ç´°è¨ºæ–­

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
        """
        self.logger.error("ğŸš¨ ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ®µéšè¨ºæ–­ã‚’å®Ÿè¡Œä¸­...")

        # ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°æƒ…å ±
        try:
            import psutil

            process = psutil.Process(monitored_process.process.pid)

            self.logger.error(f"   - ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹: {process.status()}")
            self.logger.error(f"   - CPUä½¿ç”¨ç‡: {process.cpu_percent():.2f}%")
            self.logger.error(
                f"   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {process.memory_info().rss / (1024 * 1024):.2f}MB"
            )
            self.logger.error(f"   - ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {process.num_threads()}")

            # å­ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª
            children = process.children(recursive=True)
            if children:
                self.logger.error(f"   - å­ãƒ—ãƒ­ã‚»ã‚¹æ•°: {len(children)}")
                for child in children[:5]:  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
                    try:
                        self.logger.error(f"     - å­PID {child.pid}: {child.status()}")
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass

        except (ImportError, psutil.NoSuchProcess, psutil.AccessDenied):
            self.logger.error("   - ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°æƒ…å ±ã®å–å¾—ã«å¤±æ•—")

        # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™ã®è©³ç´°
        if monitored_process.deadlock_indicators:
            self.logger.error(
                f"   - ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™æ•°: {len(monitored_process.deadlock_indicators)}"
            )
            for indicator in monitored_process.deadlock_indicators[-3:]:  # æœ€æ–°3å€‹
                self.logger.error(
                    f"     - {indicator.deadlock_type.value}: {indicator.details}"
                )

    def _log_long_running_analysis(
        self, monitored_process: MonitoredProcess, elapsed: float
    ) -> None:
        """
        é•·æ™‚é–“å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹ã®åˆ†æãƒ­ã‚°

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            elapsed: çµŒéæ™‚é–“
        """
        if self.detailed_logging:
            analysis = []

            # å®Ÿè¡Œæ™‚é–“ã®åˆ†æ
            if elapsed > 600:  # 10åˆ†ä»¥ä¸Š
                analysis.append(
                    "â° é•·æ™‚é–“å®Ÿè¡Œä¸­ - Dockeré€šä¿¡ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã®å¯èƒ½æ€§"
                )
            elif elapsed > 300:  # 5åˆ†ä»¥ä¸Š
                analysis.append("â±ï¸  é€šå¸¸ã‚ˆã‚Šé•·ã„å®Ÿè¡Œæ™‚é–“")

            # å‡ºåŠ›æ´»å‹•ã®åˆ†æ
            if (
                len(monitored_process.stdout_lines) == 0
                and len(monitored_process.stderr_lines) == 0
            ):
                analysis.append("ğŸ”‡ å‡ºåŠ›ãªã— - ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒãƒ³ã‚°ã—ã¦ã„ã‚‹å¯èƒ½æ€§")
            elif time.time() - monitored_process.last_activity > 120:  # 2åˆ†é–“æ´»å‹•ãªã—
                analysis.append("ğŸ’¤ é•·æ™‚é–“éã‚¢ã‚¯ãƒ†ã‚£ãƒ–")

            if analysis:
                self.logger.info(f"   ğŸ“ˆ é•·æ™‚é–“å®Ÿè¡Œåˆ†æ: {'; '.join(analysis)}")

    def _record_final_metrics(
        self, monitored_process: MonitoredProcess, total_duration: float
    ) -> None:
        """
        æœ€çµ‚çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            total_duration: ç·å®Ÿè¡Œæ™‚é–“
        """
        self._performance_metrics.update(
            {
                "total_duration_seconds": total_duration,
                "stdout_lines_total": len(monitored_process.stdout_lines),
                "stderr_lines_total": len(monitored_process.stderr_lines),
                "deadlock_indicators_count": len(monitored_process.deadlock_indicators),
                "force_killed": monitored_process.force_killed,
                "resource_snapshots_count": len(self._resource_snapshots),
            }
        )

        if self.detailed_logging:
            self.logger.info(
                f"ğŸ“Š æœ€çµ‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹: "
                f"{json.dumps(self._performance_metrics, ensure_ascii=False)}"
            )

    def _clear_output_buffers(self, monitored_process: MonitoredProcess) -> None:
        """
        å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
        """
        try:
            # ãƒ‘ã‚¤ãƒ—ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¯ãƒ­ãƒ¼ã‚º
            if (
                monitored_process.process.stdout
                and not monitored_process.process.stdout.closed
            ):
                monitored_process.process.stdout.close()
            if (
                monitored_process.process.stderr
                and not monitored_process.process.stderr.closed
            ):
                monitored_process.process.stderr.close()

            self.logger.debug("å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.debug(f"å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def get_performance_metrics(self) -> Dict[str, Any]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—

        Returns:
            Dict[str, Any]: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        return {
            "performance_metrics": self._performance_metrics.copy(),
            "resource_snapshots": self._resource_snapshots.copy(),
            "monitoring_config": {
                "deadlock_detection_interval": self.deadlock_detection_interval,
                "activity_timeout": self.activity_timeout,
                "warning_timeout": self.warning_timeout,
                "escalation_timeout": self.escalation_timeout,
                "heartbeat_interval": self.heartbeat_interval,
            },
        }

    def _start_deadlock_detection(self, monitored_process: MonitoredProcess) -> None:
        """ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹"""
        if self._monitoring_active:
            return

        self._monitoring_active = True
        self._stop_event.clear()
        self._monitor_thread = threading.Thread(
            target=self._deadlock_detection_loop,
            args=(monitored_process,),
            name="ProcessMonitor-DeadlockDetection",
            daemon=True,
        )
        self._monitor_thread.start()

    def _stop_deadlock_detection(self) -> None:
        """ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åœæ­¢"""
        self._monitoring_active = False
        self._stop_event.set()

        if self._monitor_thread and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=2.0)

    def _deadlock_detection_loop(self, monitored_process: MonitoredProcess) -> None:
        """ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        while self._monitoring_active and not self._stop_event.is_set():
            try:
                # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                new_indicators = self.detect_deadlock_conditions(monitored_process)
                monitored_process.deadlock_indicators.extend(new_indicators)

                # æ–°ã—ã„ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™ãŒã‚ã‚Œã°ãƒ­ã‚°å‡ºåŠ›
                for indicator in new_indicators:
                    self.logger.warning(
                        f"ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™ã‚’æ¤œå‡º: {indicator.deadlock_type.value}"
                    )

                # æŒ‡å®šé–“éš”ã§å¾…æ©Ÿ
                self._stop_event.wait(self.deadlock_detection_interval)

            except Exception as e:
                self.logger.error(f"ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                time.sleep(1.0)

    def _is_thread_responsive(self, thread: threading.Thread) -> bool:
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå¿œç­”ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            thread: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã‚¹ãƒ¬ãƒƒãƒ‰

        Returns:
            bool: å¿œç­”ã—ã¦ã„ã‚‹å ´åˆTrue
        """
        if not thread.is_alive():
            return False

        # ç°¡å˜ãªå¿œç­”æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿè£…ã¯ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¹ãƒ¬ãƒƒãƒ‰å›ºæœ‰ã®å¿œç­”æ€§æŒ‡æ¨™ã‚’ä½¿ç”¨ã™ã‚‹
        return True

    def _log_heartbeat(
        self, monitored_process: MonitoredProcess, elapsed_seconds: int
    ) -> None:
        """
        ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒ­ã‚°ã‚’å‡ºåŠ›

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            elapsed_seconds: çµŒéæ™‚é–“ï¼ˆç§’ï¼‰
        """
        process_info = {
            "pid": monitored_process.process.pid,
            "elapsed_seconds": elapsed_seconds,
            "return_code": monitored_process.process.poll(),
            "stdout_lines": len(monitored_process.stdout_lines),
            "stderr_lines": len(monitored_process.stderr_lines),
            "deadlock_indicators": len(monitored_process.deadlock_indicators),
        }

        self.logger.info(
            f"ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ: {elapsed_seconds}ç§’çµŒé | {json.dumps(process_info, ensure_ascii=False)}"
        )

    def _cleanup_threads(self, monitored_process: MonitoredProcess) -> None:
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

        Args:
            monitored_process: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
        """
        threads_to_cleanup = []
        if monitored_process.stdout_thread:
            threads_to_cleanup.append(monitored_process.stdout_thread)
        if monitored_process.stderr_thread:
            threads_to_cleanup.append(monitored_process.stderr_thread)

        for thread in threads_to_cleanup:
            if thread.is_alive():
                self.logger.debug(f"ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿ: {thread.name}")
                thread.join(timeout=2.0)
                if thread.is_alive():
                    self.logger.warning(f"ã‚¹ãƒ¬ãƒƒãƒ‰ãŒçµ‚äº†ã—ã¾ã›ã‚“ã§ã—ãŸ: {thread.name}")


class EnhancedActWrapper(ActWrapper):
    """
    è¨ºæ–­æ©Ÿèƒ½ã‚’æŒã¤æ”¹è‰¯ã•ã‚ŒãŸActWrapper
    ã‚ˆã‚Šè‰¯ã„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã€ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    def __init__(
        self,
        working_directory: Optional[str] = None,
        *,
        config: Optional[Dict[str, Any]] = None,
        logger: Optional[ActionsLogger] = None,
        execution_tracer: Optional[ExecutionTracer] = None,
        diagnostic_service: Optional[DiagnosticService] = None,
        enable_diagnostics: bool = True,
        enable_performance_monitoring: bool = True,
        deadlock_detection_interval: float = 10.0,
        activity_timeout: float = 60.0,
        performance_monitoring_interval: float = 0.5,
    ) -> None:
        """
        EnhancedActWrapperã‚’åˆæœŸåŒ–

        Args:
            working_directory: ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            config: è¨­å®šæƒ…å ±
            logger: ãƒ­ã‚°å‡ºåŠ›ç”¨ã®ãƒ­ã‚¬ãƒ¼
            execution_tracer: å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚µãƒ¼
            diagnostic_service: è¨ºæ–­ã‚µãƒ¼ãƒ“ã‚¹
            enable_diagnostics: è¨ºæ–­æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
            enable_performance_monitoring: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
            deadlock_detection_interval: ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã®é–“éš”ï¼ˆç§’ï¼‰
            activity_timeout: ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            performance_monitoring_interval: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®é–“éš”ï¼ˆç§’ï¼‰
        """
        super().__init__(
            working_directory=working_directory,
            config=config,
            logger=logger,
            execution_tracer=execution_tracer,
        )

        self.diagnostic_service = diagnostic_service or DiagnosticService(
            logger=self.logger
        )
        self.enable_diagnostics = enable_diagnostics
        self.enable_performance_monitoring = enable_performance_monitoring

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®åˆæœŸåŒ–
        self.performance_monitor = (
            PerformanceMonitor(
                logger=self.logger, monitoring_interval=performance_monitoring_interval
            )
            if enable_performance_monitoring
            else None
        )

        self.process_monitor = ProcessMonitor(
            logger=self.logger,
            deadlock_detection_interval=deadlock_detection_interval,
            activity_timeout=activity_timeout,
        )

        # Dockerçµ±åˆãƒã‚§ãƒƒã‚«ãƒ¼ã‚’è¿½åŠ 
        self.docker_integration_checker = DockerIntegrationChecker(logger=self.logger)
        self._docker_connection_verified = False
        self._docker_retry_count = 3

        # ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—æ¤œå‡ºå™¨ã‚’è¿½åŠ 
        self.hangup_detector = HangupDetector(
            logger=self.logger,
            diagnostic_service=self.diagnostic_service,
            execution_tracer=execution_tracer,
        )

        # è‡ªå‹•å¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¿½åŠ 
        self.auto_recovery = AutoRecovery(
            logger=self.logger,
            docker_checker=self.docker_integration_checker,
            max_recovery_attempts=3,
            recovery_timeout=60.0,
            enable_fallback_mode=True,
        )

    def run_workflow_with_diagnostics(
        self,
        workflow_file: Optional[str] = None,
        event: Optional[str] = None,
        job: Optional[str] = None,
        dry_run: bool = False,
        verbose: bool = False,
        env_vars: Optional[Dict[str, str]] = None,
        pre_execution_diagnostics: bool = True,
    ) -> DetailedResult:
        """
        è¨ºæ–­æ©Ÿèƒ½ä»˜ãã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ

        Args:
            workflow_file: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            job: ã‚¸ãƒ§ãƒ–å
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
            verbose: è©³ç´°ãƒ­ã‚°
            env_vars: ç’°å¢ƒå¤‰æ•°
            pre_execution_diagnostics: å®Ÿè¡Œå‰è¨ºæ–­ã‚’è¡Œã†ã‹ã©ã†ã‹

        Returns:
            DetailedResult: è©³ç´°ãªå®Ÿè¡Œçµæœ
        """
        start_time = time.time()
        diagnostic_results = []

        try:
            # å®Ÿè¡Œå‰è¨ºæ–­
            if self.enable_diagnostics and pre_execution_diagnostics:
                self.logger.info("å®Ÿè¡Œå‰è¨ºæ–­ã‚’é–‹å§‹ã—ã¾ã™...")
                health_report = self.diagnostic_service.run_comprehensive_health_check()
                diagnostic_results.extend(health_report.results)

                # Dockerçµ±åˆãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
                docker_check_results = self._verify_docker_integration_with_retry()
                if not docker_check_results["overall_success"]:
                    self.logger.error("Dockerçµ±åˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
                    recommendations = self.docker_integration_checker.generate_docker_fix_recommendations(
                        docker_check_results
                    )

                    return DetailedResult(
                        success=False,
                        returncode=-2,
                        stdout="",
                        stderr=f"Dockerçµ±åˆã‚¨ãƒ©ãƒ¼: {docker_check_results['summary']}\næ¨å¥¨ä¿®æ­£:\n"
                        + "\n".join(recommendations),
                        command="Dockerçµ±åˆãƒã‚§ãƒƒã‚¯",
                        execution_time_ms=(time.time() - start_time) * 1000,
                        diagnostic_results=diagnostic_results,
                    )

                # é‡å¤§ãªã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯å®Ÿè¡Œã‚’ä¸­æ­¢
                if health_report.has_errors:
                    error_messages = [
                        result.message
                        for result in health_report.results
                        if result.status == DiagnosticStatus.ERROR
                    ]
                    return DetailedResult(
                        success=False,
                        returncode=-1,
                        stdout="",
                        stderr=f"å®Ÿè¡Œå‰è¨ºæ–­ã§ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {'; '.join(error_messages)}",
                        command="è¨ºæ–­ãƒã‚§ãƒƒã‚¯",
                        execution_time_ms=0.0,
                        diagnostic_results=diagnostic_results,
                    )

            # æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ã§å®Ÿè¡Œ
            result = self._run_workflow_with_enhanced_monitoring(
                workflow_file=workflow_file,
                event=event,
                job=job,
                dry_run=dry_run,
                verbose=verbose,
                env_vars=env_vars,
            )

            execution_time_ms = (time.time() - start_time) * 1000

            return DetailedResult(
                success=result["success"],
                returncode=result["returncode"],
                stdout=result["stdout"],
                stderr=result["stderr"],
                command=result["command"],
                execution_time_ms=execution_time_ms,
                diagnostic_results=diagnostic_results,
                deadlock_indicators=result.get("deadlock_indicators", []),
                process_monitoring_data=result.get("process_monitoring_data", {}),
                hang_analysis=result.get("hang_analysis"),
                performance_metrics=result.get("performance_metrics"),
                bottlenecks_detected=result.get("bottlenecks_detected", []),
                optimization_opportunities=result.get("optimization_opportunities", []),
            )

        except Exception as e:
            execution_time_ms = (time.time() - start_time) * 1000
            self.logger.error(
                f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
            )

            return DetailedResult(
                success=False,
                returncode=-1,
                stdout="",
                stderr=f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}",
                command="ä¸æ˜",
                execution_time_ms=execution_time_ms,
                diagnostic_results=diagnostic_results,
            )

    def _run_workflow_with_enhanced_monitoring(
        self,
        workflow_file: Optional[str] = None,
        event: Optional[str] = None,
        job: Optional[str] = None,
        dry_run: bool = False,
        verbose: bool = False,
        env_vars: Optional[Dict[str, str]] = None,
    ) -> Dict[str, Any]:
        """
        æ”¹è‰¯ã•ã‚ŒãŸç›£è¦–æ©Ÿèƒ½ã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ä»˜ãï¼‰

        Args:
            workflow_file: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            job: ã‚¸ãƒ§ãƒ–å
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
            verbose: è©³ç´°ãƒ­ã‚°
            env_vars: ç’°å¢ƒå¤‰æ•°

        Returns:
            Dict[str, Any]: å®Ÿè¡Œçµæœ
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å…ƒã®å®Ÿè£…ã‚’ä½¿ç”¨
        if self._mock_mode:
            return super().run_workflow(
                workflow_file=workflow_file,
                event=event,
                job=job,
                dry_run=dry_run,
                verbose=verbose,
                env_vars=env_vars,
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹
        performance_metrics = None
        bottlenecks_detected = []
        optimization_opportunities = []

        if self.performance_monitor:
            self.performance_monitor.start_monitoring()
            self.performance_monitor.start_stage("workflow_initialization")

        # å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–‹å§‹
        trace_id = f"enhanced_act_workflow_{int(time.time() * 1000)}"
        self.execution_tracer.start_trace(trace_id)

        try:
            # åˆæœŸåŒ–æ®µéš
            self.execution_tracer.set_stage(
                ExecutionStage.INITIALIZATION,
                {
                    "workflow_file": workflow_file,
                    "job": job,
                    "dry_run": dry_run,
                    "verbose": verbose,
                    "enhanced_monitoring": True,
                    "performance_monitoring": self.performance_monitor is not None,
                },
            )

            # ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰æ®µéš
            if self.performance_monitor:
                self.performance_monitor.end_stage()
                self.performance_monitor.start_stage("command_building")

            cmd = self._build_command(
                workflow_file, event, job, dry_run, verbose, env_vars
            )
            process_env = self._build_process_env(event, env_vars)

            self.logger.info(f"æ”¹è‰¯ã•ã‚ŒãŸactã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}")

            # ãƒ—ãƒ­ã‚»ã‚¹ä½œæˆæ®µéš
            if self.performance_monitor:
                self.performance_monitor.end_stage()
                self.performance_monitor.start_stage("subprocess_creation")

            # å®‰å…¨ãªã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ä½œæˆ
            monitored_process = self._create_monitored_subprocess(cmd, process_env)

            # Dockeræ“ä½œã‚’è¨˜éŒ²
            if self.performance_monitor:
                self.performance_monitor.record_docker_operation(
                    "subprocess_creation", str(monitored_process.process.pid)
                )

            # å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ®µéš
            if self.performance_monitor:
                self.performance_monitor.end_stage()
                self.performance_monitor.start_stage("output_streaming")

            # å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’å®‰å…¨ã«å‡¦ç†
            self._handle_output_streaming_safely(monitored_process)

            # ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–æ®µéš
            if self.performance_monitor:
                self.performance_monitor.end_stage()
                self.performance_monitor.start_stage("process_monitoring")

            # ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†
            timed_out, deadlock_indicators = (
                self.process_monitor.monitor_with_heartbeat(
                    monitored_process, self._timeout_seconds
                )
            )

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®å‡¦ç†
            if timed_out:
                self.logger.error(
                    "ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯ã—ã¾ã—ãŸ"
                )

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢ã—ã¦åˆ†æ
                if self.performance_monitor:
                    self.performance_monitor.end_stage()
                    self.performance_monitor.stop_monitoring()

                    detailed_analysis = self.performance_monitor.get_detailed_analysis()
                    performance_metrics = detailed_analysis["performance_summary"]
                    bottlenecks_detected = detailed_analysis["bottlenecks"]
                    optimization_opportunities = detailed_analysis[
                        "optimization_opportunities"
                    ]

                self.process_monitor.force_cleanup_on_timeout(monitored_process)

                # ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—åˆ†æ
                hang_analysis = self._analyze_hang_condition(
                    monitored_process, deadlock_indicators
                )

                # è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                diagnostic_results = []
                if self.enable_diagnostics:
                    health_report = (
                        self.diagnostic_service.run_comprehensive_health_check()
                    )
                    diagnostic_results = health_report.results

                error_report = self.hangup_detector.generate_detailed_error_report(
                    hangup_analysis=hang_analysis,
                    diagnostic_results=diagnostic_results,
                    execution_trace=self.execution_tracer.get_current_trace(),
                )

                return {
                    "success": False,
                    "returncode": -1,
                    "stdout": "".join(monitored_process.stdout_lines),
                    "stderr": "".join(monitored_process.stderr_lines)
                    or "Execution timeout or deadlock",
                    "command": " ".join(cmd),
                    "deadlock_indicators": deadlock_indicators,
                    "hang_analysis": hang_analysis,
                    "error_report": error_report,
                    "performance_metrics": performance_metrics,
                    "bottlenecks_detected": bottlenecks_detected,
                    "optimization_opportunities": optimization_opportunities,
                    "process_monitoring_data": {
                        "force_killed": monitored_process.force_killed,
                        "execution_time": time.time() - monitored_process.start_time,
                    },
                }

            # å®Œäº†æ®µéš
            if self.performance_monitor:
                self.performance_monitor.end_stage()
                self.performance_monitor.start_stage("completion_processing")

            # æ­£å¸¸çµ‚äº†ã®å‡¦ç†
            return_code = monitored_process.process.returncode or 0
            stdout_text = "".join(monitored_process.stdout_lines)
            stderr_text = "".join(monitored_process.stderr_lines)

            if return_code != 0:
                self.logger.error(f"actå®Ÿè¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ (returncode={return_code})")
                self.execution_tracer.set_stage(ExecutionStage.FAILED)
            else:
                self.execution_tracer.set_stage(ExecutionStage.COMPLETED)

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢ã—ã¦åˆ†æ
            if self.performance_monitor:
                self.performance_monitor.end_stage()
                self.performance_monitor.stop_monitoring()

                detailed_analysis = self.performance_monitor.get_detailed_analysis()
                performance_metrics = detailed_analysis["performance_summary"]
                bottlenecks_detected = detailed_analysis["bottlenecks"]
                optimization_opportunities = detailed_analysis[
                    "optimization_opportunities"
                ]

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
                if self.logger.verbose and performance_metrics:
                    self.logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµæœ:")
                    self.logger.info(
                        f"   å®Ÿè¡Œæ™‚é–“: {performance_metrics.get('total_execution_time_ms', 0):.2f}ms"
                    )
                    self.logger.info(
                        f"   ãƒ”ãƒ¼ã‚¯CPU: {performance_metrics.get('cpu_usage', {}).get('peak', 0):.1f}%"
                    )
                    self.logger.info(
                        f"   ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {performance_metrics.get('memory_usage', {}).get('peak_mb', 0):.1f}MB"
                    )
                    self.logger.info(
                        f"   Dockeræ“ä½œæ•°: {performance_metrics.get('docker_operations', {}).get('total_count', 0)}"
                    )

                    if bottlenecks_detected:
                        self.logger.warning(
                            f"âš ï¸  æ¤œå‡ºã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {len(bottlenecks_detected)}å€‹"
                        )
                        for bottleneck in bottlenecks_detected[:3]:  # æœ€åˆã®3å€‹ã®ã¿è¡¨ç¤º
                            self.logger.warning(
                                f"   - {bottleneck['type']}: {bottleneck['description']}"
                            )

                    if optimization_opportunities:
                        self.logger.info(
                            f"ğŸ’¡ æœ€é©åŒ–æ©Ÿä¼š: {len(optimization_opportunities)}å€‹"
                        )
                        for opportunity in optimization_opportunities[
                            :2
                        ]:  # æœ€åˆã®2å€‹ã®ã¿è¡¨ç¤º
                            self.logger.info(
                                f"   - {opportunity['title']}: {opportunity['estimated_improvement']}"
                            )

            return {
                "success": return_code == 0,
                "returncode": return_code,
                "stdout": stdout_text,
                "stderr": stderr_text,
                "command": " ".join(cmd),
                "deadlock_indicators": deadlock_indicators,
                "performance_metrics": performance_metrics,
                "bottlenecks_detected": bottlenecks_detected,
                "optimization_opportunities": optimization_opportunities,
                "process_monitoring_data": {
                    "force_killed": monitored_process.force_killed,
                    "execution_time": time.time() - monitored_process.start_time,
                },
            }

        finally:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯åœæ­¢
            if self.performance_monitor and self.performance_monitor.is_monitoring():
                self.performance_monitor.stop_monitoring()

            # ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’çµ‚äº†
            final_trace = self.execution_tracer.end_trace()
            if final_trace and self.logger.verbose:
                self.logger.debug(f"æ”¹è‰¯ã•ã‚ŒãŸå®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹å®Œäº†: {final_trace.trace_id}")

    def _build_command(
        self,
        workflow_file: Optional[str],
        event: Optional[str],
        job: Optional[str],
        dry_run: bool,
        verbose: bool,
        env_vars: Optional[Dict[str, str]],
    ) -> List[str]:
        """
        å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰

        Args:
            workflow_file: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            job: ã‚¸ãƒ§ãƒ–å
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
            verbose: è©³ç´°ãƒ­ã‚°
            env_vars: ç’°å¢ƒå¤‰æ•°

        Returns:
            List[str]: æ§‹ç¯‰ã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰
        """
        cmd = [self.act_binary]
        cmd.extend(self._compose_runner_flags())

        if workflow_file:
            cmd.extend(["-W", workflow_file])
        if job:
            cmd.extend(["-j", job])
        if dry_run:
            cmd.append("--dryrun")
        if verbose:
            cmd.append("--verbose")

        env_args = self._compose_env_args(env_vars)
        cmd.extend(env_args)

        return cmd

    def _create_monitored_subprocess(
        self, cmd: List[str], process_env: Dict[str, str]
    ) -> MonitoredProcess:
        """
        ç›£è¦–å¯¾è±¡ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®‰å…¨ã«ä½œæˆ

        Args:
            cmd: å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
            process_env: ãƒ—ãƒ­ã‚»ã‚¹ç’°å¢ƒå¤‰æ•°

        Returns:
            MonitoredProcess: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹

        Raises:
            RuntimeError: ãƒ—ãƒ­ã‚»ã‚¹ä½œæˆã«å¤±æ•—ã—ãŸå ´åˆ
        """
        self.execution_tracer.set_stage(ExecutionStage.SUBPROCESS_CREATION)

        try:
            # ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½œæˆ
            process = subprocess.Popen(
                cmd,
                cwd=self.working_directory,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                env=process_env,
                # ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã—ã¦ã€å­ãƒ—ãƒ­ã‚»ã‚¹ã‚‚å«ã‚ã¦åˆ¶å¾¡ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
                preexec_fn=os.setsid if hasattr(os, "setsid") else None,
            )

            monitored_process = MonitoredProcess(
                process=process, command=cmd, start_time=time.time()
            )

            # ãƒ—ãƒ­ã‚»ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–‹å§‹
            self.execution_tracer.trace_subprocess_execution(
                cmd, process, str(self.working_directory)
            )

            self.logger.debug(f"ç›£è¦–å¯¾è±¡ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ: PID {process.pid}")
            return monitored_process

        except (OSError, subprocess.SubprocessError) as exc:
            self.logger.error(f"ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {exc}")
            self.execution_tracer.set_stage(ExecutionStage.FAILED)
            raise RuntimeError(f"ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}") from exc

    def _handle_output_streaming_safely(
        self, monitored_process: MonitoredProcess
    ) -> Dict[str, Any]:
        """
        å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’å®‰å…¨ã«å‡¦ç†

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹

        Returns:
            Dict[str, Any]: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµæœ
        """
        self.execution_tracer.set_stage(ExecutionStage.OUTPUT_STREAMING)

        def _safe_stream_output(
            pipe: Any,
            collector: List[str],
            label: str,
            monitored_process: MonitoredProcess,
        ) -> None:
            """å®‰å…¨ãªå‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–¢æ•°"""
            try:
                if pipe is None:
                    return

                with pipe:
                    for raw_line in pipe:
                        collector.append(raw_line)
                        monitored_process.last_activity = time.time()

                        line = raw_line.rstrip("\n")
                        if line and self.logger.verbose:
                            self.logger.debug(f"[{label}] {line}")

            except Exception as e:
                self.logger.error(
                    f"å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({label}): {e}"
                )
                # ã‚¨ãƒ©ãƒ¼ã‚’ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã«è¨˜éŒ²
                collector.append(f"[ERROR] ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}\n")

        # æ¨™æº–å‡ºåŠ›ã‚¹ãƒ¬ãƒƒãƒ‰
        if monitored_process.process.stdout:
            stdout_thread = threading.Thread(
                target=_safe_stream_output,
                args=(
                    monitored_process.process.stdout,
                    monitored_process.stdout_lines,
                    "act stdout",
                    monitored_process,
                ),
                name="EnhancedActWrapper-StdoutStream",
                daemon=True,
            )
            stdout_thread.start()
            monitored_process.stdout_thread = stdout_thread

            # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–‹å§‹
            self.execution_tracer.track_thread_lifecycle(
                stdout_thread, "_safe_stream_output"
            )

        # æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰
        if monitored_process.process.stderr:
            stderr_thread = threading.Thread(
                target=_safe_stream_output,
                args=(
                    monitored_process.process.stderr,
                    monitored_process.stderr_lines,
                    "act stderr",
                    monitored_process,
                ),
                name="EnhancedActWrapper-StderrStream",
                daemon=True,
            )
            stderr_thread.start()
            monitored_process.stderr_thread = stderr_thread

            # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–‹å§‹
            self.execution_tracer.track_thread_lifecycle(
                stderr_thread, "_safe_stream_output"
            )

        return {
            "stdout_thread_started": monitored_process.stdout_thread is not None,
            "stderr_thread_started": monitored_process.stderr_thread is not None,
        }

    def _analyze_hang_condition(
        self,
        monitored_process: MonitoredProcess,
        deadlock_indicators: List[DeadlockIndicator],
    ) -> HangupAnalysis:
        """
        ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—æ¡ä»¶ã‚’åŒ…æ‹¬çš„ã«åˆ†æ

        Args:
            monitored_process: ç›£è¦–å¯¾è±¡ãƒ—ãƒ­ã‚»ã‚¹
            deadlock_indicators: ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™

        Returns:
            HangupAnalysis: åŒ…æ‹¬çš„ãªãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—åˆ†æçµæœ
        """
        self.logger.info("åŒ…æ‹¬çš„ãªãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")

        try:
            # ç¾åœ¨ã®å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—
            current_trace = self.execution_tracer.get_current_trace()

            # è¨ºæ–­çµæœã‚’å–å¾—
            diagnostic_results = []
            if self.enable_diagnostics:
                health_report = self.diagnostic_service.run_comprehensive_health_check()
                diagnostic_results = health_report.results

            # HangupDetectorã‚’ä½¿ç”¨ã—ã¦åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
            hangup_analysis = self.hangup_detector.analyze_hangup_conditions(
                execution_trace=current_trace, diagnostic_results=diagnostic_results
            )

            # å¾“æ¥ã®ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æŒ‡æ¨™ã‚’è¿½åŠ æƒ…å ±ã¨ã—ã¦å«ã‚ã‚‹
            if deadlock_indicators:
                legacy_info = {
                    "legacy_deadlock_indicators": [
                        {
                            "type": indicator.deadlock_type.value,
                            "detected_at": indicator.detected_at,
                            "severity": indicator.severity,
                            "details": indicator.details,
                            "recommendations": indicator.recommendations,
                        }
                        for indicator in deadlock_indicators
                    ]
                }
                hangup_analysis.system_state.update(legacy_info)

            # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’è¿½åŠ 
            process_info = {
                "monitored_process": {
                    "pid": monitored_process.process.pid,
                    "command": " ".join(monitored_process.command),
                    "execution_time": time.time() - monitored_process.start_time,
                    "force_killed": monitored_process.force_killed,
                    "stdout_lines": len(monitored_process.stdout_lines),
                    "stderr_lines": len(monitored_process.stderr_lines),
                }
            }
            hangup_analysis.execution_context.update(process_info)

            self.logger.info(
                f"ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—åˆ†æå®Œäº†: {len(hangup_analysis.issues)}å€‹ã®å•é¡Œã‚’æ¤œå‡º"
            )

            return hangup_analysis

        except Exception as e:
            self.logger.error(f"ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªåˆ†æçµæœã‚’è¿”ã™
            fallback_analysis = HangupAnalysis(
                analysis_id=f"fallback_analysis_{int(time.time() * 1000)}"
            )
            fallback_analysis.system_state = {
                "analysis_error": str(e),
                "process_pid": monitored_process.process.pid,
                "execution_time": time.time() - monitored_process.start_time,
            }
            return fallback_analysis

    def create_debug_bundle_for_hangup(
        self, error_report: ErrorReport, output_directory: Optional[Path] = None
    ) -> Optional[DebugBundle]:
        """
        ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—å•é¡Œç”¨ã®ãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ«ã‚’ä½œæˆ

        Args:
            error_report: ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
            output_directory: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            Optional[DebugBundle]: ä½œæˆã•ã‚ŒãŸãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ«ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            self.logger.info("ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—å•é¡Œç”¨ã®ãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ«ã‚’ä½œæˆä¸­...")

            debug_bundle = self.hangup_detector.create_debug_bundle(
                error_report=error_report,
                output_directory=output_directory,
                include_logs=True,
                include_system_info=True,
                include_docker_info=True,
            )

            if debug_bundle.bundle_path:
                self.logger.info(
                    f"ãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ: {debug_bundle.bundle_path} "
                    f"({debug_bundle.total_size_bytes} bytes)"
                )
            else:
                self.logger.error("ãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

            return debug_bundle

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ«ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None

    def _verify_docker_integration_with_retry(self) -> Dict[str, Any]:
        """
        ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§Dockerçµ±åˆã‚’æ¤œè¨¼

        Returns:
            Dict[str, Any]: Dockerçµ±åˆãƒã‚§ãƒƒã‚¯çµæœ
        """
        if self._docker_connection_verified:
            self.logger.debug("Dockeræ¥ç¶šã¯æ—¢ã«æ¤œè¨¼æ¸ˆã¿ã§ã™")
            return {
                "overall_success": True,
                "summary": "Dockerçµ±åˆã¯æ­£å¸¸ã§ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ï¼‰",
            }

        self.logger.info("Dockerçµ±åˆã‚’ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§æ¤œè¨¼ä¸­...")

        # åŒ…æ‹¬çš„ãªDockerãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
        check_results = self.docker_integration_checker.run_comprehensive_docker_check()

        if check_results["overall_success"]:
            self._docker_connection_verified = True
            self.logger.success("Dockerçµ±åˆæ¤œè¨¼å®Œäº†: æ­£å¸¸ âœ“")
        else:
            self.logger.error(f"Dockerçµ±åˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {check_results['summary']}")

            # ä¿®æ­£æ¨å¥¨äº‹é …ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
            recommendations = (
                self.docker_integration_checker.generate_docker_fix_recommendations(
                    check_results
                )
            )
            self.logger.info("Dockerçµ±åˆã®ä¿®æ­£æ¨å¥¨äº‹é …:")
            for rec in recommendations:
                self.logger.info(f"  {rec}")

        return check_results

    def _ensure_docker_connection(self) -> bool:
        """
        Dockeræ¥ç¶šã‚’ç¢ºä¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒªãƒˆãƒ©ã‚¤ï¼‰

        Returns:
            bool: Dockeræ¥ç¶šãŒç¢ºä¿ã§ããŸã‹ã©ã†ã‹
        """
        if self._docker_connection_verified:
            return True

        self.logger.debug("Dockeræ¥ç¶šã‚’ç¢ºä¿ä¸­...")

        # Docker daemonæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        connection_result = (
            self.docker_integration_checker.test_docker_daemon_connection_with_retry()
        )

        if connection_result.status == DockerConnectionStatus.CONNECTED:
            self._docker_connection_verified = True
            self.logger.debug(
                f"Dockeræ¥ç¶šç¢ºä¿æˆåŠŸ: {connection_result.response_time_ms:.1f}ms"
            )
            return True
        else:
            self.logger.error(f"Dockeræ¥ç¶šç¢ºä¿å¤±æ•—: {connection_result.message}")
            return False

    def reset_docker_connection_cache(self) -> None:
        """
        Dockeræ¥ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆ
        """
        self._docker_connection_verified = False
        self.logger.debug("Dockeræ¥ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")

    def run_workflow_with_auto_recovery(
        self,
        workflow_file: Optional[str] = None,
        event: Optional[str] = None,
        job: Optional[str] = None,
        dry_run: bool = False,
        verbose: bool = False,
        env_vars: Optional[Dict[str, str]] = None,
        enable_recovery: bool = True,
        max_recovery_attempts: int = 2,
    ) -> DetailedResult:
        """
        è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ä»˜ãã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ

        Args:
            workflow_file: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            job: ã‚¸ãƒ§ãƒ–å
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰
            verbose: è©³ç´°ãƒ­ã‚°
            env_vars: ç’°å¢ƒå¤‰æ•°
            enable_recovery: è‡ªå‹•å¾©æ—§ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
            max_recovery_attempts: æœ€å¤§å¾©æ—§è©¦è¡Œå›æ•°

        Returns:
            DetailedResult: è©³ç´°ãªå®Ÿè¡Œçµæœï¼ˆå¾©æ—§æƒ…å ±ã‚’å«ã‚€ï¼‰
        """
        self.logger.info("è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ä»˜ããƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã‚’é–‹å§‹...")

        recovery_session: Optional[RecoverySession] = None
        fallback_result: Optional[FallbackExecutionResult] = None
        primary_execution_failed = False

        # æœ€åˆã«é€šå¸¸ã®å®Ÿè¡Œã‚’è©¦è¡Œ
        try:
            result = self.run_workflow_with_diagnostics(
                workflow_file=workflow_file,
                event=event,
                job=job,
                dry_run=dry_run,
                verbose=verbose,
                env_vars=env_vars,
            )

            if result.success:
                self.logger.success("ãƒ—ãƒ©ã‚¤ãƒãƒªå®Ÿè¡ŒãŒæˆåŠŸã—ã¾ã—ãŸ")
                return result
            else:
                primary_execution_failed = True
                self.logger.warning(
                    "ãƒ—ãƒ©ã‚¤ãƒãƒªå®Ÿè¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ - è‡ªå‹•å¾©æ—§ã‚’é–‹å§‹ã—ã¾ã™"
                )

        except Exception as e:
            primary_execution_failed = True
            self.logger.error(
                f"ãƒ—ãƒ©ã‚¤ãƒãƒªå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)} - è‡ªå‹•å¾©æ—§ã‚’é–‹å§‹ã—ã¾ã™"
            )

            # ã‚¨ãƒ©ãƒ¼æ™‚ã®åŸºæœ¬çµæœã‚’ä½œæˆ
            result = DetailedResult(
                success=False,
                returncode=-1,
                stdout="",
                stderr=f"ãƒ—ãƒ©ã‚¤ãƒãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}",
                command="",
                execution_time_ms=0,
            )

        # è‡ªå‹•å¾©æ—§ãŒæœ‰åŠ¹ã§ã€ãƒ—ãƒ©ã‚¤ãƒãƒªå®Ÿè¡ŒãŒå¤±æ•—ã—ãŸå ´åˆ
        if enable_recovery and primary_execution_failed:
            self.logger.info("è‡ªå‹•å¾©æ—§å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")

            # å¾©æ—§è©¦è¡Œå›æ•°ã®ãƒ«ãƒ¼ãƒ—
            for attempt in range(max_recovery_attempts):
                self.logger.info(f"å¾©æ—§è©¦è¡Œ {attempt + 1}/{max_recovery_attempts}")

                try:
                    # åŒ…æ‹¬çš„å¾©æ—§å‡¦ç†ã‚’å®Ÿè¡Œ
                    workflow_path = Path(workflow_file) if workflow_file else None
                    original_command = self._build_act_command(
                        workflow_file=workflow_file,
                        event=event,
                        job=job,
                        dry_run=dry_run,
                        verbose=verbose,
                        env_vars=env_vars,
                    )

                    recovery_session = self.auto_recovery.run_comprehensive_recovery(
                        failed_process=None,  # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ãŒãªã„å ´åˆ
                        workflow_file=workflow_path,
                        original_command=original_command,
                    )

                    # å¾©æ—§å¾Œã«å†å®Ÿè¡Œã‚’è©¦è¡Œ
                    if recovery_session.overall_success:
                        self.logger.info("å¾©æ—§ãŒæˆåŠŸã—ã¾ã—ãŸ - å†å®Ÿè¡Œã‚’è©¦è¡Œã—ã¾ã™")

                        try:
                            retry_result = self.run_workflow_with_diagnostics(
                                workflow_file=workflow_file,
                                event=event,
                                job=job,
                                dry_run=dry_run,
                                verbose=verbose,
                                env_vars=env_vars,
                            )

                            if retry_result.success:
                                self.logger.success("å¾©æ—§å¾Œã®å†å®Ÿè¡ŒãŒæˆåŠŸã—ã¾ã—ãŸ")
                                # å¾©æ—§æƒ…å ±ã‚’çµæœã«è¿½åŠ 
                                retry_result.diagnostic_results.append(
                                    DiagnosticResult(
                                        component="auto_recovery",
                                        status=DiagnosticStatus.OK,
                                        message=f"è‡ªå‹•å¾©æ—§æˆåŠŸ (è©¦è¡Œ {attempt + 1}/{max_recovery_attempts})",
                                        details={
                                            "recovery_session_id": recovery_session.session_id
                                        },
                                        recommendations=[
                                            "è‡ªå‹•å¾©æ—§ã«ã‚ˆã‚Šå•é¡ŒãŒè§£æ±ºã•ã‚Œã¾ã—ãŸ"
                                        ],
                                    )
                                )
                                return retry_result

                        except Exception as e:
                            self.logger.warning(f"å¾©æ—§å¾Œã®å†å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼: {str(e)}")

                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œã‚’è©¦è¡Œ
                    if workflow_path and self.auto_recovery.enable_fallback_mode:
                        self.logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œã‚’è©¦è¡Œã—ã¾ã™")
                        fallback_result = self.auto_recovery.execute_fallback_mode(
                            workflow_path, original_command
                        )

                        if fallback_result.success:
                            self.logger.success("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡ŒãŒæˆåŠŸã—ã¾ã—ãŸ")
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ DetailedResult ã«å¤‰æ›
                            result = DetailedResult(
                                success=True,
                                returncode=fallback_result.returncode,
                                stdout=fallback_result.stdout,
                                stderr=fallback_result.stderr,
                                command=" ".join(original_command),
                                execution_time_ms=fallback_result.execution_time_ms,
                            )
                            result.diagnostic_results.append(
                                DiagnosticResult(
                                    component="auto_recovery_fallback",
                                    status=DiagnosticStatus.OK,
                                    message=f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡ŒæˆåŠŸ: {fallback_result.fallback_method}",
                                    details={
                                        "fallback_method": fallback_result.fallback_method,
                                        "limitations": fallback_result.limitations,
                                        "warnings": fallback_result.warnings,
                                    },
                                    recommendations=[
                                        "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ"
                                    ],
                                )
                            )
                            return result

                except Exception as e:
                    self.logger.error(
                        f"å¾©æ—§è©¦è¡Œ {attempt + 1} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                    )

                # æœ€å¾Œã®è©¦è¡Œã§ãªã„å ´åˆã¯å°‘ã—å¾…æ©Ÿ
                if attempt < max_recovery_attempts - 1:
                    self.logger.info("æ¬¡ã®å¾©æ—§è©¦è¡Œã¾ã§å¾…æ©Ÿä¸­...")
                    time.sleep(5)

            # å…¨ã¦ã®å¾©æ—§è©¦è¡ŒãŒå¤±æ•—
            self.logger.error("å…¨ã¦ã®è‡ªå‹•å¾©æ—§è©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ")

        # å¾©æ—§æƒ…å ±ã‚’çµæœã«è¿½åŠ 
        if recovery_session:
            result.diagnostic_results.append(
                DiagnosticResult(
                    component="auto_recovery",
                    status=DiagnosticStatus.ERROR
                    if not recovery_session.overall_success
                    else DiagnosticStatus.WARNING,
                    message=f"è‡ªå‹•å¾©æ—§{'æˆåŠŸ' if recovery_session.overall_success else 'å¤±æ•—'}: {len(recovery_session.attempts)}å€‹ã®å¾©æ—§æ“ä½œå®Ÿè¡Œ",
                    details={
                        "recovery_session_id": recovery_session.session_id,
                        "total_attempts": len(recovery_session.attempts),
                        "successful_attempts": sum(
                            1
                            for a in recovery_session.attempts
                            if a.status.value == "success"
                        ),
                        "fallback_activated": recovery_session.fallback_mode_activated,
                    },
                    recommendations=[
                        "è‡ªå‹•å¾©æ—§ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€å•é¡ŒãŒå®Œå…¨ã«è§£æ±ºã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                        "æ‰‹å‹•ã§ã®å•é¡Œèª¿æŸ»ã‚’æ¨å¥¨ã—ã¾ã™",
                    ],
                )
            )

        if fallback_result:
            result.diagnostic_results.append(
                DiagnosticResult(
                    component="fallback_execution",
                    status=DiagnosticStatus.WARNING
                    if fallback_result.success
                    else DiagnosticStatus.ERROR,
                    message=f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ{'æˆåŠŸ' if fallback_result.success else 'å¤±æ•—'}: {fallback_result.fallback_method}",
                    details={
                        "fallback_method": fallback_result.fallback_method,
                        "limitations": fallback_result.limitations,
                        "warnings": fallback_result.warnings,
                    },
                    recommendations=["ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œã®åˆ¶é™äº‹é …ã‚’ç¢ºèªã—ã¦ãã ã•ã„"],
                )
            )

        return result

    def _build_act_command(
        self,
        workflow_file: Optional[str] = None,
        event: Optional[str] = None,
        job: Optional[str] = None,
        dry_run: bool = False,
        verbose: bool = False,
        env_vars: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        """
        actã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰

        Args:
            workflow_file: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            job: ã‚¸ãƒ§ãƒ–å
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰
            verbose: è©³ç´°ãƒ­ã‚°
            env_vars: ç’°å¢ƒå¤‰æ•°

        Returns:
            List[str]: actã‚³ãƒãƒ³ãƒ‰ã®å¼•æ•°ãƒªã‚¹ãƒˆ
        """
        cmd = ["act"]

        if event:
            cmd.append(event)

        if job:
            cmd.extend(["-j", job])

        if workflow_file:
            cmd.extend(["-W", workflow_file])

        if dry_run:
            cmd.append("--dry-run")

        if verbose:
            cmd.append("--verbose")

        if env_vars:
            for key, value in env_vars.items():
                cmd.extend(["--env", f"{key}={value}"])

        return cmd

    def get_auto_recovery_statistics(self) -> Dict[str, Any]:
        """
        è‡ªå‹•å¾©æ—§çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

        Returns:
            Dict[str, Any]: è‡ªå‹•å¾©æ—§çµ±è¨ˆæƒ…å ±
        """
        return self.auto_recovery.get_recovery_statistics()

    def export_performance_metrics(
        self, output_path: Path, format: str = "json", include_raw_data: bool = True
    ) -> bool:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            format: å‡ºåŠ›å½¢å¼ ("json" ã®ã¿ã‚µãƒãƒ¼ãƒˆ)
            include_raw_data: ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹

        Returns:
            bool: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæˆåŠŸæ™‚True
        """
        if not self.performance_monitor:
            self.logger.warning(
                "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãŒç„¡åŠ¹ã®ãŸã‚ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“"
            )
            return False

        try:
            return self.performance_monitor.export_metrics(
                output_path=output_path, format=format
            )
        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_performance_summary(self) -> Optional[Dict[str, Any]]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—

        Returns:
            Optional[Dict[str, Any]]: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ï¼ˆç›£è¦–ãŒç„¡åŠ¹ã®å ´åˆã¯Noneï¼‰
        """
        if not self.performance_monitor:
            return None

        try:
            return self.performance_monitor.get_performance_summary()
        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_bottleneck_analysis(self) -> List[Dict[str, Any]]:
        """
        ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã‚’å–å¾—

        Returns:
            List[Dict[str, Any]]: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        """
        if not self.performance_monitor:
            return []

        try:
            detailed_analysis = self.performance_monitor.get_detailed_analysis()
            return detailed_analysis.get("bottlenecks", [])
        except Exception as e:
            self.logger.error(f"ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def get_optimization_opportunities(self) -> List[Dict[str, Any]]:
        """
        æœ€é©åŒ–æ©Ÿä¼šã‚’å–å¾—

        Returns:
            List[Dict[str, Any]]: æœ€é©åŒ–æ©Ÿä¼šã®ãƒªã‚¹ãƒˆ
        """
        if not self.performance_monitor:
            return []

        try:
            detailed_analysis = self.performance_monitor.get_detailed_analysis()
            return detailed_analysis.get("optimization_opportunities", [])
        except Exception as e:
            self.logger.error(f"æœ€é©åŒ–æ©Ÿä¼šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def analyze_performance_trends(
        self, historical_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ

        Args:
            historical_data: éå»ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            Dict[str, Any]: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ
        """
        if not historical_data:
            return {"error": "å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}

        try:
            # å®Ÿè¡Œæ™‚é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            execution_times = [
                data.get("performance_summary", {}).get("total_execution_time_ms", 0)
                for data in historical_data
                if data.get("performance_summary")
            ]

            # CPUä½¿ç”¨ç‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            cpu_peaks = [
                data.get("performance_summary", {}).get("cpu_usage", {}).get("peak", 0)
                for data in historical_data
                if data.get("performance_summary", {}).get("cpu_usage")
            ]

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            memory_peaks = [
                data.get("performance_summary", {})
                .get("memory_usage", {})
                .get("peak_mb", 0)
                for data in historical_data
                if data.get("performance_summary", {}).get("memory_usage")
            ]

            # Dockeræ“ä½œæ•°ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            docker_ops = [
                data.get("performance_summary", {})
                .get("docker_operations", {})
                .get("total_count", 0)
                for data in historical_data
                if data.get("performance_summary", {}).get("docker_operations")
            ]

            import statistics

            trend_analysis = {
                "data_points": len(historical_data),
                "execution_time_trend": {
                    "average_ms": statistics.mean(execution_times)
                    if execution_times
                    else 0,
                    "median_ms": statistics.median(execution_times)
                    if execution_times
                    else 0,
                    "min_ms": min(execution_times) if execution_times else 0,
                    "max_ms": max(execution_times) if execution_times else 0,
                    "trend": "improving"
                    if len(execution_times) >= 2
                    and execution_times[-1] < execution_times[0]
                    else "stable",
                },
                "cpu_usage_trend": {
                    "average_percent": statistics.mean(cpu_peaks) if cpu_peaks else 0,
                    "peak_percent": max(cpu_peaks) if cpu_peaks else 0,
                    "trend": "stable",
                },
                "memory_usage_trend": {
                    "average_mb": statistics.mean(memory_peaks) if memory_peaks else 0,
                    "peak_mb": max(memory_peaks) if memory_peaks else 0,
                    "trend": "stable",
                },
                "docker_operations_trend": {
                    "average_count": statistics.mean(docker_ops) if docker_ops else 0,
                    "max_count": max(docker_ops) if docker_ops else 0,
                    "trend": "stable",
                },
            }

            # ãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ¤å®š
            if len(execution_times) >= 3:
                recent_avg = statistics.mean(execution_times[-3:])
                older_avg = (
                    statistics.mean(execution_times[:-3])
                    if len(execution_times) > 3
                    else execution_times[0]
                )

                if recent_avg < older_avg * 0.9:
                    trend_analysis["execution_time_trend"]["trend"] = "improving"
                elif recent_avg > older_avg * 1.1:
                    trend_analysis["execution_time_trend"]["trend"] = "degrading"

            # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
            recommendations = []

            if trend_analysis["execution_time_trend"]["trend"] == "degrading":
                recommendations.append(
                    "å®Ÿè¡Œæ™‚é–“ãŒæ‚ªåŒ–å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"
                )

            if trend_analysis["cpu_usage_trend"]["average_percent"] > 80:
                recommendations.append(
                    "CPUä½¿ç”¨ç‡ãŒç¶™ç¶šçš„ã«é«˜ã„ã§ã™ã€‚ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
                )

            if trend_analysis["memory_usage_trend"]["average_mb"] > 1000:
                recommendations.append(
                    "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ã§ã™ã€‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
                )

            if trend_analysis["docker_operations_trend"]["average_count"] > 100:
                recommendations.append(
                    "Dockeræ“ä½œãŒå¤šã„ã§ã™ã€‚æ“ä½œã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
                )

            trend_analysis["recommendations"] = recommendations

            return trend_analysis

        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
