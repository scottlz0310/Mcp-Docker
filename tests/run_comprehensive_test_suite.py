#!/usr/bin/env python3
"""
GitHub Actions Simulator - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
========================================================

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ã‚¿ã‚¹ã‚¯18ã§å®Ÿè£…ã•ã‚ŒãŸåŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

å®Ÿè¡Œå†…å®¹:
1. é…å¸ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…¨æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´åˆæ€§ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‹•ä½œæ¤œè¨¼
3. ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ãƒ†ã‚¹ãƒˆ
4. çµ±åˆãƒ†ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼

ä½¿ç”¨æ–¹æ³•:
    python tests/run_comprehensive_test_suite.py [options]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    --quick         ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
    --full          ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
    --report        è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    --output FILE   çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
"""

import argparse
import json
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict


class ComprehensiveTestRunner:
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self, project_root: Path, verbose: bool = False):
        self.project_root = project_root
        self.verbose = verbose
        self.test_results = {}
        self.start_time = None
        self.end_time = None

    def run_comprehensive_tests(self, quick_mode: bool = False) -> Dict:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        self.start_time = datetime.now()

        print("ğŸš€ GitHub Actions Simulator åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
        print(f"ğŸ“… é–‹å§‹æ™‚åˆ»: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {self.project_root}")
        print()

        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®šç¾©
        test_suites = [
            {
                "name": "é…å¸ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ",
                "module": "tests.test_comprehensive_distribution",
                "essential": True,
                "timeout": 300,
            },
            {
                "name": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´åˆæ€§ãƒ†ã‚¹ãƒˆ",
                "module": "tests.test_documentation_consistency",
                "essential": True,
                "timeout": 180,
            },
            {
                "name": "ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ãƒ†ã‚¹ãƒˆ",
                "module": "tests.test_end_to_end_user_experience",
                "essential": True,
                "timeout": 600,
            },
            {
                "name": "åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆ",
                "module": "tests.test_comprehensive_integration_suite",
                "essential": not quick_mode,
                "timeout": 900,
            },
            {
                "name": "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ¤œè¨¼ãƒ†ã‚¹ãƒˆ",
                "module": "tests.test_template_validation",
                "essential": True,
                "timeout": 240,
            },
        ]

        # å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if quick_mode:
            test_suites = [
                suite for suite in test_suites if suite.get("essential", True)
            ]
            print("âš¡ ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
        else:
            print("ğŸ” ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã§å®Ÿè¡Œä¸­...")

        print(f"ğŸ“‹ å®Ÿè¡Œäºˆå®šãƒ†ã‚¹ãƒˆæ•°: {len(test_suites)}")
        print()

        # å„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        for i, suite in enumerate(test_suites, 1):
            print(f"[{i}/{len(test_suites)}] {suite['name']} ã‚’å®Ÿè¡Œä¸­...")

            result = self._run_test_suite(
                suite["module"], suite["name"], suite.get("timeout", 300)
            )

            self.test_results[suite["name"]] = result

            # çµæœã®è¡¨ç¤º
            if result["success"]:
                print(f"âœ… {suite['name']}: æˆåŠŸ ({result['execution_time']:.2f}ç§’)")
            else:
                print(f"âŒ {suite['name']}: å¤±æ•— - {result['error']}")

            if result.get("warnings"):
                print(f"âš ï¸  è­¦å‘Š: {len(result['warnings'])} ä»¶")

            print()

        self.end_time = datetime.now()

        # çµæœã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
        summary = self._generate_summary()

        return {
            "summary": summary,
            "detailed_results": self.test_results,
            "execution_info": {
                "start_time": self.start_time.isoformat(),
                "end_time": self.end_time.isoformat(),
                "total_duration": (self.end_time - self.start_time).total_seconds(),
                "quick_mode": quick_mode,
            },
        }

    def _run_test_suite(self, module_name: str, suite_name: str, timeout: int) -> Dict:
        """å€‹åˆ¥ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ"""
        start_time = time.time()

        try:
            # pytestã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            cmd = [
                sys.executable,
                "-m",
                "pytest",
                module_name.replace(".", "/") + ".py",
                "-v",
                "--tb=short",
                "--disable-warnings",
            ]

            if self.verbose:
                cmd.append("-s")

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=self.project_root,
            )

            execution_time = time.time() - start_time

            # çµæœã®è§£æ
            success = result.returncode == 0

            # è­¦å‘Šã®æŠ½å‡º
            warnings = []
            if result.stdout:
                lines = result.stdout.split("\n")
                for line in lines:
                    if "warning" in line.lower() or "warn" in line.lower():
                        warnings.append(line.strip())

            # ãƒ†ã‚¹ãƒˆçµ±è¨ˆã®æŠ½å‡º
            stats = self._extract_test_statistics(result.stdout)

            return {
                "success": success,
                "execution_time": execution_time,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "warnings": warnings,
                "statistics": stats,
                "timeout_used": timeout,
            }

        except subprocess.TimeoutExpired:
            execution_time = time.time() - start_time
            return {
                "success": False,
                "execution_time": execution_time,
                "error": f"ãƒ†ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ ({timeout}ç§’)",
                "timeout_expired": True,
            }

        except Exception as e:
            execution_time = time.time() - start_time
            return {
                "success": False,
                "execution_time": execution_time,
                "error": f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}",
                "exception": True,
            }

    def _extract_test_statistics(self, output: str) -> Dict:
        """ãƒ†ã‚¹ãƒˆçµ±è¨ˆæƒ…å ±ã®æŠ½å‡º"""
        stats = {"total_tests": 0, "passed": 0, "failed": 0, "skipped": 0, "errors": 0}

        try:
            lines = output.split("\n")
            for line in lines:
                # pytest ã®çµæœè¡Œã‚’æ¢ã™
                if "passed" in line and (
                    "failed" in line or "error" in line or "skipped" in line
                ):
                    # ä¾‹: "5 passed, 2 failed, 1 skipped in 10.5s"
                    import re

                    passed_match = re.search(r"(\d+)\s+passed", line)
                    if passed_match:
                        stats["passed"] = int(passed_match.group(1))

                    failed_match = re.search(r"(\d+)\s+failed", line)
                    if failed_match:
                        stats["failed"] = int(failed_match.group(1))

                    skipped_match = re.search(r"(\d+)\s+skipped", line)
                    if skipped_match:
                        stats["skipped"] = int(skipped_match.group(1))

                    error_match = re.search(r"(\d+)\s+error", line)
                    if error_match:
                        stats["errors"] = int(error_match.group(1))

                    stats["total_tests"] = (
                        stats["passed"]
                        + stats["failed"]
                        + stats["skipped"]
                        + stats["errors"]
                    )
                    break

        except Exception:
            # çµ±è¨ˆæŠ½å‡ºã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
            pass

        return stats

    def _generate_summary(self) -> Dict:
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        total_suites = len(self.test_results)
        successful_suites = sum(
            1 for result in self.test_results.values() if result["success"]
        )
        failed_suites = total_suites - successful_suites

        total_execution_time = sum(
            result["execution_time"] for result in self.test_results.values()
        )

        # å…¨ä½“çµ±è¨ˆã®é›†è¨ˆ
        total_stats = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0,
        }

        for result in self.test_results.values():
            stats = result.get("statistics", {})
            for key in total_stats:
                total_stats[key] += stats.get(key, 0)

        # è­¦å‘Šã®é›†è¨ˆ
        total_warnings = sum(
            len(result.get("warnings", [])) for result in self.test_results.values()
        )

        # æˆåŠŸç‡ã®è¨ˆç®—
        success_rate = (
            (successful_suites / total_suites * 100) if total_suites > 0 else 0
        )
        test_success_rate = (
            (total_stats["passed"] / total_stats["total_tests"] * 100)
            if total_stats["total_tests"] > 0
            else 0
        )

        return {
            "overall_success": failed_suites == 0,
            "success_rate": success_rate,
            "test_success_rate": test_success_rate,
            "suite_statistics": {
                "total_suites": total_suites,
                "successful_suites": successful_suites,
                "failed_suites": failed_suites,
            },
            "test_statistics": total_stats,
            "execution_summary": {
                "total_execution_time": total_execution_time,
                "average_suite_time": total_execution_time / total_suites
                if total_suites > 0
                else 0,
                "total_warnings": total_warnings,
            },
        }

    def generate_report(self, results: Dict, format_type: str = "text") -> str:
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        if format_type == "json":
            return json.dumps(results, indent=2, ensure_ascii=False)

        # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ
        summary = results["summary"]
        execution_info = results["execution_info"]

        report_lines = [
            "=" * 80,
            "GitHub Actions Simulator - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 80,
            "",
            f"ğŸ• å®Ÿè¡Œæ™‚åˆ»: {execution_info['start_time']} - {execution_info['end_time']}",
            f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {execution_info['total_duration']:.2f}ç§’",
            f"ğŸƒ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {'ã‚¯ã‚¤ãƒƒã‚¯' if execution_info['quick_mode'] else 'ãƒ•ãƒ«'}",
            "",
            "ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼:",
            f"  ğŸ¯ å…¨ä½“æˆåŠŸç‡: {summary['success_rate']:.1f}%",
            f"  ğŸ§ª ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {summary['test_success_rate']:.1f}%",
            f"  âœ… æˆåŠŸã‚¹ã‚¤ãƒ¼ãƒˆ: {summary['suite_statistics']['successful_suites']}/{summary['suite_statistics']['total_suites']}",
            f"  âŒ å¤±æ•—ã‚¹ã‚¤ãƒ¼ãƒˆ: {summary['suite_statistics']['failed_suites']}",
            "",
            "ğŸ§ª ãƒ†ã‚¹ãƒˆçµ±è¨ˆ:",
            f"  ğŸ“‹ ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['test_statistics']['total_tests']}",
            f"  âœ… æˆåŠŸ: {summary['test_statistics']['passed']}",
            f"  âŒ å¤±æ•—: {summary['test_statistics']['failed']}",
            f"  â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {summary['test_statistics']['skipped']}",
            f"  ğŸš¨ ã‚¨ãƒ©ãƒ¼: {summary['test_statistics']['errors']}",
            f"  âš ï¸ è­¦å‘Š: {summary['execution_summary']['total_warnings']}",
            "",
            "ğŸ“‹ è©³ç´°çµæœ:",
            "-" * 80,
        ]

        # å„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®è©³ç´°çµæœ
        for suite_name, result in results["detailed_results"].items():
            status_icon = "âœ…" if result["success"] else "âŒ"
            report_lines.append(f"{status_icon} {suite_name}")
            report_lines.append(f"  â±ï¸ å®Ÿè¡Œæ™‚é–“: {result['execution_time']:.2f}ç§’")

            if result.get("statistics"):
                stats = result["statistics"]
                if stats["total_tests"] > 0:
                    report_lines.append(
                        f"  ğŸ§ª ãƒ†ã‚¹ãƒˆ: {stats['passed']}/{stats['total_tests']} æˆåŠŸ"
                    )

            if result.get("warnings"):
                report_lines.append(f"  âš ï¸ è­¦å‘Š: {len(result['warnings'])} ä»¶")

            if not result["success"]:
                error_msg = result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                report_lines.append(f"  ğŸš¨ ã‚¨ãƒ©ãƒ¼: {error_msg}")

            report_lines.append("")

        # æ¨å¥¨äº‹é …
        if summary["suite_statistics"]["failed_suites"] > 0:
            report_lines.extend(
                [
                    "ğŸ’¡ æ¨å¥¨äº‹é …:",
                    "  1. å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "  2. ä¾å­˜é–¢ä¿‚ã¨ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "  3. å¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„",
                    "",
                ]
            )

        if summary["execution_summary"]["total_warnings"] > 0:
            report_lines.extend(
                [
                    "âš ï¸ è­¦å‘Šã«ã¤ã„ã¦:",
                    "  è­¦å‘Šã¯æ©Ÿèƒ½ã«å½±éŸ¿ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€ç¢ºèªã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™",
                    "",
                ]
            )

        report_lines.extend(
            [
                "=" * 80,
                f"ãƒ†ã‚¹ãƒˆå®Œäº† - å…¨ä½“æˆåŠŸ: {'âœ…' if summary['overall_success'] else 'âŒ'}",
                "=" * 80,
            ]
        )

        return "\n".join(report_lines)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="GitHub Actions Simulator åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python tests/run_comprehensive_test_suite.py                    # ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  python tests/run_comprehensive_test_suite.py --quick           # ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  python tests/run_comprehensive_test_suite.py --report          # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
  python tests/run_comprehensive_test_suite.py --output report.txt  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        """,
    )

    parser.add_argument(
        "--quick", action="store_true", help="ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œï¼ˆå¿…é ˆãƒ†ã‚¹ãƒˆã®ã¿ï¼‰"
    )

    parser.add_argument(
        "--full", action="store_true", help="ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"
    )

    parser.add_argument("--report", action="store_true", help="è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ")

    parser.add_argument("--output", type=str, help="çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›")

    parser.add_argument(
        "--format",
        choices=["text", "json"],
        default="text",
        help="å‡ºåŠ›å½¢å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: textï¼‰",
    )

    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›")

    args = parser.parse_args()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®æ±ºå®š
    project_root = Path(__file__).parent.parent

    try:
        # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã®åˆæœŸåŒ–
        runner = ComprehensiveTestRunner(project_root, verbose=args.verbose)

        # ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        quick_mode = args.quick and not args.full
        results = runner.run_comprehensive_tests(quick_mode=quick_mode)

        # ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        if args.report or args.output:
            report = runner.generate_report(results, args.format)

            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    f.write(report)
                print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {args.output}")
            else:
                print(report)
        else:
            # ç°¡æ½”ãªã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            summary = results["summary"]
            print("ğŸ“Š ãƒ†ã‚¹ãƒˆå®Œäº†ã‚µãƒãƒªãƒ¼:")
            print(f"  å…¨ä½“æˆåŠŸ: {'âœ…' if summary['overall_success'] else 'âŒ'}")
            print(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
            print(f"  å®Ÿè¡Œæ™‚é–“: {results['execution_info']['total_duration']:.2f}ç§’")

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰ã®è¨­å®š
        exit_code = 0 if results["summary"]["overall_success"] else 1
        sys.exit(exit_code)

    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)

    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
