#!/usr/bin/env python3
"""
GitHub Actions Simulator - åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚¿ã‚¹ã‚¯15: çµ±åˆãƒ†ã‚¹ãƒˆã¨æœ€çµ‚æ¤œè¨¼ã®å®Œå…¨å®Ÿè£…

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œã—ã€æœ€çµ‚çš„ãªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™:
1. å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆ
2. å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å®‰å®šæ€§ã®æ¤œè¨¼
4. æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ
5. æœ€çµ‚çš„ãªè¦ä»¶æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
"""

import json
import subprocess
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List

# ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from test_comprehensive_integration import ComprehensiveIntegrationTest
from test_end_to_end_validation import EndToEndValidationTest
from test_performance_stability_validation import PerformanceStabilityValidator

from services.actions.logger import ActionsLogger


class ComprehensiveTestRunner:
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.logger = ActionsLogger(verbose=True, debug=False)
        self.test_results = {}
        self.start_time = time.time()
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)

    def run_existing_test_suite(self) -> Dict[str, any]:
        """æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œä¸­...")

        results = {
            "pytest_results": {},
            "bats_results": {},
            "integration_script_results": {},
        }

        # 1. pytest ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        self.logger.info("pytest ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        pytest_files = [
            "tests/test_diagnostic_service.py",
            "tests/test_execution_tracer.py",
            "tests/test_enhanced_act_wrapper.py",
            "tests/test_hangup_detector.py",
            "tests/test_auto_recovery.py",
            "tests/test_hangup_scenarios_comprehensive.py",
            "tests/test_logger.py",
            "tests/test_workflow_parser.py",
            "tests/test_output.py",
            "tests/test_expression.py",
        ]

        for test_file in pytest_files:
            if Path(test_file).exists():
                try:
                    self.logger.info(f"å®Ÿè¡Œä¸­: {test_file}")
                    result = subprocess.run(
                        [sys.executable, "-m", "pytest", test_file, "-v", "--tb=short"],
                        capture_output=True,
                        text=True,
                        timeout=300,  # 5åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    )

                    results["pytest_results"][test_file] = {
                        "return_code": result.returncode,
                        "success": result.returncode == 0,
                        "stdout": result.stdout,
                        "stderr": result.stderr,
                    }

                    if result.returncode == 0:
                        self.logger.info(f"âœ… {test_file} æˆåŠŸ")
                    else:
                        self.logger.error(
                            f"âŒ {test_file} å¤±æ•— (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})"
                        )

                except subprocess.TimeoutExpired:
                    self.logger.error(f"â° {test_file} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    results["pytest_results"][test_file] = {
                        "return_code": -1,
                        "success": False,
                        "error": "timeout",
                    }
                except Exception as e:
                    self.logger.error(f"âŒ {test_file} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                    results["pytest_results"][test_file] = {
                        "return_code": -1,
                        "success": False,
                        "error": str(e),
                    }
            else:
                self.logger.warning(f"âš ï¸ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_file}")

        # 2. Bats ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        self.logger.info("Bats ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        bats_files = [
            "tests/test_docker_build.bats",
            "tests/test_services.bats",
            "tests/test_integration.bats",
            "tests/test_actions_simulator.bats",
        ]

        if self._is_bats_available():
            for bats_file in bats_files:
                if Path(bats_file).exists():
                    try:
                        self.logger.info(f"å®Ÿè¡Œä¸­: {bats_file}")
                        result = subprocess.run(
                            ["bats", bats_file],
                            capture_output=True,
                            text=True,
                            timeout=300,
                        )

                        results["bats_results"][bats_file] = {
                            "return_code": result.returncode,
                            "success": result.returncode == 0,
                            "stdout": result.stdout,
                            "stderr": result.stderr,
                        }

                        if result.returncode == 0:
                            self.logger.info(f"âœ… {bats_file} æˆåŠŸ")
                        else:
                            self.logger.error(f"âŒ {bats_file} å¤±æ•—")

                    except subprocess.TimeoutExpired:
                        self.logger.error(f"â° {bats_file} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                        results["bats_results"][bats_file] = {
                            "return_code": -1,
                            "success": False,
                            "error": "timeout",
                        }
                    except Exception as e:
                        self.logger.error(f"âŒ {bats_file} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                        results["bats_results"][bats_file] = {
                            "return_code": -1,
                            "success": False,
                            "error": str(e),
                        }
        else:
            self.logger.warning(
                "âš ï¸ Bats ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Bats ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )

        # 3. çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
        integration_script = Path("tests/integration_test.sh")
        if integration_script.exists():
            try:
                self.logger.info("çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œä¸­...")
                result = subprocess.run(
                    ["bash", str(integration_script)],
                    capture_output=True,
                    text=True,
                    timeout=600,  # 10åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                )

                results["integration_script_results"] = {
                    "return_code": result.returncode,
                    "success": result.returncode == 0,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                }

                if result.returncode == 0:
                    self.logger.info("âœ… çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ æˆåŠŸ")
                else:
                    self.logger.error("âŒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ å¤±æ•—")

            except subprocess.TimeoutExpired:
                self.logger.error("â° çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                results["integration_script_results"] = {
                    "return_code": -1,
                    "success": False,
                    "error": "timeout",
                }
            except Exception as e:
                self.logger.error(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                results["integration_script_results"] = {
                    "return_code": -1,
                    "success": False,
                    "error": str(e),
                }

        return results

    def run_comprehensive_integration_tests(self) -> Dict[str, any]:
        """åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")

        try:
            integration_tester = ComprehensiveIntegrationTest()
            return integration_tester.run_all_tests()
        except Exception as e:
            self.logger.error(f"åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e), "summary": {"overall_success": False}}

    def run_end_to_end_validation(self) -> Dict[str, any]:
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")

        try:
            e2e_tester = EndToEndValidationTest()
            return e2e_tester.run_comprehensive_end_to_end_tests()
        except Exception as e:
            self.logger.error(f"ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¤œè¨¼ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e), "summary": {"overall_success": False}}

    def run_performance_stability_validation(self) -> Dict[str, any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å®‰å®šæ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å®‰å®šæ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")

        try:
            perf_validator = PerformanceStabilityValidator()
            return perf_validator.run_comprehensive_performance_stability_tests()
        except Exception as e:
            self.logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å®‰å®šæ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e), "summary": {"overall_success": False}}

    def validate_requirements(self) -> Dict[str, bool]:
        """è¦ä»¶5.1-5.4ã®æ¤œè¨¼"""
        self.logger.info("è¦ä»¶æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")

        validation_results = {}

        # Requirement 5.1: æ§˜ã€…ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æˆåŠŸå®Ÿè¡Œ
        e2e_results = self.test_results.get("end_to_end_validation", {})
        req_5_1 = (
            e2e_results.get("requirements_validation", {}).get("requirement_5_1", False)
            and e2e_results.get("summary", {}).get(
                "simulation_execution_success_rate", 0
            )
            >= 0.8
        )
        validation_results["requirement_5_1"] = req_5_1

        # Requirement 5.2: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚·ãƒŠãƒªã‚ªã®é©åˆ‡ãªå‡¦ç†
        req_5_2 = (
            e2e_results.get("requirements_validation", {}).get("requirement_5_2", False)
            and e2e_results.get("summary", {}).get("timeout_handling_success_rate", 0)
            >= 0.8
        )
        validation_results["requirement_5_2"] = req_5_2

        # Requirement 5.3: å®‰å®šæ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        perf_results = self.test_results.get("performance_stability_validation", {})
        integration_results = self.test_results.get("comprehensive_integration", {})

        req_5_3 = (
            perf_results.get("requirements_validation", {}).get(
                "requirement_5_3_stability", False
            )
            and perf_results.get("requirements_validation", {}).get(
                "requirement_5_3_performance", False
            )
            and perf_results.get("requirements_validation", {}).get(
                "requirement_5_3_memory_management", False
            )
            and integration_results.get("summary", {}).get(
                "performance_acceptable", False
            )
        )
        validation_results["requirement_5_3"] = req_5_3

        # Requirement 5.4: æ§˜ã€…ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­å®šã®å‡¦ç†
        req_5_4 = (
            e2e_results.get("requirements_validation", {}).get("requirement_5_4", False)
            and e2e_results.get("summary", {}).get("workflow_parsing_success_rate", 0)
            >= 0.9
        )
        validation_results["requirement_5_4"] = req_5_4

        return validation_results

    def generate_final_report(self) -> Dict[str, any]:
        """æœ€çµ‚çš„ãªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        total_time = time.time() - self.start_time

        # è¦ä»¶æ¤œè¨¼
        requirements_validation = self.validate_requirements()

        # æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®æˆåŠŸç‡è¨ˆç®—
        existing_tests = self.test_results.get("existing_test_suite", {})
        pytest_success_rate = self._calculate_test_success_rate(
            existing_tests.get("pytest_results", {})
        )
        bats_success_rate = self._calculate_test_success_rate(
            existing_tests.get("bats_results", {})
        )
        integration_script_success = existing_tests.get(
            "integration_script_results", {}
        ).get("success", False)

        # æ–°è¦ãƒ†ã‚¹ãƒˆã®æˆåŠŸåˆ¤å®š
        comprehensive_success = (
            self.test_results.get("comprehensive_integration", {})
            .get("summary", {})
            .get("overall_success", False)
        )
        e2e_success = (
            self.test_results.get("end_to_end_validation", {})
            .get("summary", {})
            .get("overall_success", False)
        )
        perf_success = (
            self.test_results.get("performance_stability_validation", {})
            .get("summary", {})
            .get("overall_success", False)
        )

        # ç·åˆæˆåŠŸåˆ¤å®š
        all_requirements_met = all(requirements_validation.values())
        all_new_tests_passed = all([comprehensive_success, e2e_success, perf_success])
        existing_tests_acceptable = (
            pytest_success_rate >= 0.8 and integration_script_success
        )

        overall_success = (
            all_requirements_met and all_new_tests_passed and existing_tests_acceptable
        )

        return {
            "test_execution_summary": {
                "start_time": datetime.fromtimestamp(
                    self.start_time, tz=timezone.utc
                ).isoformat(),
                "end_time": datetime.now(timezone.utc).isoformat(),
                "total_duration_seconds": total_time,
                "total_duration_minutes": total_time / 60,
            },
            "requirements_validation": requirements_validation,
            "test_suite_results": {
                "existing_tests": {
                    "pytest_success_rate": pytest_success_rate,
                    "bats_success_rate": bats_success_rate,
                    "integration_script_success": integration_script_success,
                },
                "new_integration_tests": {
                    "comprehensive_integration_success": comprehensive_success,
                    "end_to_end_validation_success": e2e_success,
                    "performance_stability_success": perf_success,
                },
            },
            "detailed_results": self.test_results,
            "summary": {
                "overall_success": overall_success,
                "all_requirements_met": all_requirements_met,
                "all_new_tests_passed": all_new_tests_passed,
                "existing_tests_acceptable": existing_tests_acceptable,
                "task_15_completed": overall_success,
            },
            "recommendations": self._generate_recommendations(),
        }

    def _calculate_test_success_rate(self, test_results: Dict) -> float:
        """ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ã‚’è¨ˆç®—"""
        if not test_results:
            return 0.0

        successful = sum(
            1 for result in test_results.values() if result.get("success", False)
        )
        total = len(test_results)

        return successful / total if total > 0 else 0.0

    def _is_bats_available(self) -> bool:
        """BatsãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            result = subprocess.run(
                ["bats", "--version"], capture_output=True, text=True, timeout=10
            )
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
            return False

    def _generate_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # è¦ä»¶æ¤œè¨¼çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …
        requirements = self.validate_requirements()

        if not requirements.get("requirement_5_1", True):
            recommendations.append(
                "Requirement 5.1: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã®æˆåŠŸç‡ã‚’å‘ä¸Šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è§£æã®æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if not requirements.get("requirement_5_2", True):
            recommendations.append(
                "Requirement 5.2: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ã®æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡ºã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if not requirements.get("requirement_5_3", True):
            recommendations.append(
                "Requirement 5.3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å®‰å®šæ€§ã®æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ã¨ä¸¦è¡Œå‡¦ç†ã®å®‰å®šæ€§å‘ä¸Šã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if not requirements.get("requirement_5_4", True):
            recommendations.append(
                "Requirement 5.4: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­å®šã®å‡¦ç†èƒ½åŠ›å‘ä¸ŠãŒå¿…è¦ã§ã™ã€‚è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹æ–‡ã®ã‚µãƒãƒ¼ãƒˆå¼·åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        # ãƒ†ã‚¹ãƒˆçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …
        existing_tests = self.test_results.get("existing_test_suite", {})
        pytest_success_rate = self._calculate_test_success_rate(
            existing_tests.get("pytest_results", {})
        )

        if pytest_success_rate < 0.9:
            recommendations.append(
                "æ—¢å­˜ã®pytestãƒ†ã‚¹ãƒˆã®æˆåŠŸç‡ãŒä½ã„ã§ã™ã€‚å¤±æ•—ã—ã¦ã„ã‚‹ãƒ†ã‚¹ãƒˆã®åŸå› ã‚’èª¿æŸ»ã—ã€ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚"
            )

        perf_results = self.test_results.get("performance_stability_validation", {})
        if not perf_results.get("summary", {}).get(
            "memory_leak_detection_passed", True
        ):
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æ”¹å–„ã¨ãƒªã‚½ãƒ¼ã‚¹ã®é©åˆ‡ãªè§£æ”¾ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚"
            )

        if not perf_results.get("summary", {}).get(
            "concurrent_execution_stability_passed", True
        ):
            recommendations.append(
                "ä¸¦è¡Œå®Ÿè¡Œæ™‚ã®å®‰å®šæ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã¨ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆã®è§£æ±ºã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if not recommendations:
            recommendations.append(
                "å…¨ã¦ã®è¦ä»¶ã¨ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®å®Ÿè£…å“è³ªã‚’ç¶­æŒã—ã€ç¶™ç¶šçš„ãªç›£è¦–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
            )

        return recommendations

    def run_all_tests(self) -> Dict[str, any]:
        """å…¨ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info(
            "GitHub Actions Simulator åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é–‹å§‹ã—ã¾ã™..."
        )
        self.logger.info("=" * 80)

        try:
            # 1. æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
            self.logger.info("ãƒ•ã‚§ãƒ¼ã‚º 1: æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ")
            self.test_results["existing_test_suite"] = self.run_existing_test_suite()

            # 2. åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            self.logger.info("ãƒ•ã‚§ãƒ¼ã‚º 2: åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
            self.test_results["comprehensive_integration"] = (
                self.run_comprehensive_integration_tests()
            )

            # 3. ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            self.logger.info("ãƒ•ã‚§ãƒ¼ã‚º 3: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
            self.test_results["end_to_end_validation"] = (
                self.run_end_to_end_validation()
            )

            # 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å®‰å®šæ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            self.logger.info("ãƒ•ã‚§ãƒ¼ã‚º 4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å®‰å®šæ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
            self.test_results["performance_stability_validation"] = (
                self.run_performance_stability_validation()
            )

            # 5. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            self.logger.info("ãƒ•ã‚§ãƒ¼ã‚º 5: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
            final_report = self.generate_final_report()

            # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            report_file = self.output_dir / "final_integration_test_report.json"
            report_file.write_text(
                json.dumps(final_report, ensure_ascii=False, indent=2), encoding="utf-8"
            )

            self.logger.info(f"æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {report_file}")

            return final_report

        except Exception as e:
            self.logger.error(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return {
                "error": str(e),
                "summary": {"overall_success": False, "task_15_completed": False},
            }

    def print_summary_report(self, final_report: Dict[str, any]) -> None:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
        print("\n" + "=" * 80)
        print("GitHub Actions Simulator - åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆæœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)

        summary = final_report["summary"]
        requirements = final_report["requirements_validation"]
        test_suites = final_report["test_suite_results"]

        # ç·åˆçµæœ
        print(
            f"\nğŸ¯ ã‚¿ã‚¹ã‚¯15å®Œäº†çŠ¶æ³: {'âœ… å®Œäº†' if summary['task_15_completed'] else 'âŒ æœªå®Œäº†'}"
        )
        print(f"ğŸ“Š ç·åˆæˆåŠŸ: {'âœ… æˆåŠŸ' if summary['overall_success'] else 'âŒ å¤±æ•—'}")

        # è¦ä»¶æ¤œè¨¼çµæœ
        print("\nğŸ“‹ è¦ä»¶æ¤œè¨¼çµæœ:")
        print(
            f"  Requirement 5.1 (ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ): {'âœ…' if requirements['requirement_5_1'] else 'âŒ'}"
        )
        print(
            f"  Requirement 5.2 (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†): {'âœ…' if requirements['requirement_5_2'] else 'âŒ'}"
        )
        print(
            f"  Requirement 5.3 (å®‰å®šæ€§ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹): {'âœ…' if requirements['requirement_5_3'] else 'âŒ'}"
        )
        print(
            f"  Requirement 5.4 (ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­å®š): {'âœ…' if requirements['requirement_5_4'] else 'âŒ'}"
        )

        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆçµæœ
        print("\nğŸ§ª ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆçµæœ:")
        existing = test_suites["existing_tests"]
        new_tests = test_suites["new_integration_tests"]

        print("  æ—¢å­˜ãƒ†ã‚¹ãƒˆ:")
        print(f"    pytestæˆåŠŸç‡: {existing['pytest_success_rate']:.1%}")
        print(
            f"    çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {'âœ…' if existing['integration_script_success'] else 'âŒ'}"
        )

        print("  æ–°è¦çµ±åˆãƒ†ã‚¹ãƒˆ:")
        print(
            f"    åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆ: {'âœ…' if new_tests['comprehensive_integration_success'] else 'âŒ'}"
        )
        print(
            f"    ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¤œè¨¼: {'âœ…' if new_tests['end_to_end_validation_success'] else 'âŒ'}"
        )
        print(
            f"    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å®‰å®šæ€§: {'âœ…' if new_tests['performance_stability_success'] else 'âŒ'}"
        )

        # å®Ÿè¡Œæ™‚é–“
        execution = final_report["test_execution_summary"]
        print(f"\nâ±ï¸  å®Ÿè¡Œæ™‚é–“: {execution['total_duration_minutes']:.1f}åˆ†")

        # æ¨å¥¨äº‹é …
        recommendations = final_report["recommendations"]
        if recommendations:
            print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")

        print("\n" + "=" * 80)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = ComprehensiveTestRunner()

    try:
        # å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        final_report = runner.run_all_tests()

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        runner.print_summary_report(final_report)

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰ã‚’æ±ºå®š
        success = final_report["summary"]["overall_success"]
        task_completed = final_report["summary"]["task_15_completed"]

        if task_completed:
            print("ğŸ‰ ã‚¿ã‚¹ã‚¯15ã€Œçµ±åˆãƒ†ã‚¹ãƒˆã¨æœ€çµ‚æ¤œè¨¼ã€ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            return 0
        elif success:
            print("âš ï¸ ãƒ†ã‚¹ãƒˆã¯æˆåŠŸã—ã¾ã—ãŸãŒã€ä¸€éƒ¨ã®è¦ä»¶ãŒæœªé”æˆã§ã™ã€‚")
            return 1
        else:
            print(
                "âŒ çµ±åˆãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ä¸Šè¨˜ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )
            return 2

    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        return 130
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
